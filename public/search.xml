<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ESRI ArcGIS Pro Maps</title>
      <link href="/2022/12/25/GIS-MAP/esri-gis-map/"/>
      <url>/2022/12/25/GIS-MAP/esri-gis-map/</url>
      
        <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Gallaudet University is a private Federally Chartered University</p><ul><li>Established in 1857 with only twelve students</li><li>Emphasizes diversity among staff and students, including international students.</li></ul><p>Diversity has significantly increased at Gallaudet University over the years, with over 1,500 students pursuing degrees in various disciplines.</p><p>Our group were interested in learning more about Gallaudet Graduates regarding their home location and degrees obtained from Gallaudet University.</p><ul><li>Data were obtained from Gallaudet University Institutional Research.</li><li>Allowed us to dig deeper into the data uncovering some interesting facts about Gallaudet’s graduates.</li><li>This is a preliminary short-term research project for the course, and does not represent any official university research projects.</li></ul><p>The maps were created using ESRI ArcGIS Pro mapping software, which also allowed data visualization to highlight facts about Gallaudet graduates</p><p>Our data, spanning back to 2000 to 2022, is focused on the following:</p><ul><li>Home state (or country if applicable) of Gallaudet Graduates.</li><li>Population density of Gallaudet Students.</li><li>Showcasing the map of U.S. in regard to Deaf schools (Daytime &amp; Resident-based schools).</li><li>Data for international students’ home schools were not available.</li><li>Highlighting the difference between Females and Males Graduates obtaining a degree from Gallaudet University on the global map.</li><li>Showcasing countries on a global scale in regard to type of degree obtained at Gallaudet University.</li></ul><h3 id="GIS-Maps"><a href="#GIS-Maps" class="headerlink" title="GIS Maps"></a>GIS Maps</h3><div class="fj-gallery"><p><img src="/images/gis-map/World-map.jpeg" title=" "><br><img src="/images/gis-map/home-states.jpeg" title=" "><br><img src="/images/gis-map/heatmap.jpeg" title=" "><br><img src="/images/gis-map/heatmap-with-schools.jpeg" title=" "><br><img src="/images/gis-map/gender-pie-map.jpeg" title=" "><br><img src="/images/gis-map/degrees-pie-map.jpeg" title=" "><br><img src="/images/gis-map/dots-map.jpeg" title=" "><br><img src="/images/gis-map/dots-map-DMV.jpeg" title=" "></p>          </div><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Preliminary and proof-of-concept data analysis indicates that student diversity at Gallaudet University is healthy and possibly expanding.<br>Student population density is more pronounced and visible on the East Coast compared to West Coast, likely due to:</p><ul><li>Smaller state sizes.</li><li>Large population centers.</li><li>Higher number of deaf residential-based and daytime schools.</li><li>Gallaudet University likely have had a role in creation of deaf communities along the East Coast over time.</li></ul><p>Closing of Deaf residential schools for the Deaf</p><ul><li>Likely have no impact on the student diversity and enrollment at Gallaudet University.</li></ul><p>Formal researches is recommended:</p><ul><li>To determine the impacts of schools for the Deaf closing on university enrollments.</li><li>To study the student populations across the U.S. and identify any possible recruiting gaps.</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Welcome!</title>
      <link href="/2022/12/21/WelcomePage/"/>
      <url>/2022/12/21/WelcomePage/</url>
      
        <content type="html"><![CDATA[<p>Welcome to my blog, my name is Zehui. Currently I am a senior student at Gallaudet University majoring in IT. This is just a very simple blog where I could share my knowledges and experiences through completing college assignments and discovering many other new things online. I will upload some articles and useful information contents periodically. I hope you enjoy it!</p><p>You might want to do a quick search with in just few keywords to find your desired information. To do that, find a <b>magnifying-glass icon</b> on the <b>top left corner of the navigation bar.</b></p><p><img src="/images/Logo/search-icon.jpg"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Huffman Algorithm and Greedy Method</title>
      <link href="/2022/12/17/Algorithm/FinalProject/"/>
      <url>/2022/12/17/Algorithm/FinalProject/</url>
      
        <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>A specific kind of optimum algorithm called a Huffman code is frequently employed for lossless data compression in computer science. This is a method created by David A. Huffman while he was a Sc.D. student at MIT, is used to find or use such a code. It was first published in the 1952 publication “A Method for the Construction of Minimum-Redundancy Codes.”</p><p>Huffman’s approach produces what is essentially a variable-length code table for encoding a source symbol (such as a character in a file). The estimated frequency of occurrence for each potential value of the source symbol is what the algorithm uses to create this table. Commoner symbols are typically represented using fewer bits than less common symbols. If the inputs are sorted, Huffman’s approach can be applied quickly, discovering a code in a time that is proportional to the number of inputs. Even though it is not the best compression method, it was released in 1952, making it a fairly algorithm that was widely used in specific research purpose or commercial purpose such as WinZip compression tool and JPEG, PNG picture format.</p><h3 id="Step-by-Step-Explanation"><a href="#Step-by-Step-Explanation" class="headerlink" title="Step by Step Explanation"></a>Step by Step Explanation</h3><h4 id="Read-file-from-source-and-extract"><a href="#Read-file-from-source-and-extract" class="headerlink" title="Read file from source and extract"></a>Read file from source and extract</h4><p>In order to describe the Huffman method in detail, step by step, I used the programming language Python. Note that this is not the greatest way for optimizing these codes such that they can take up more space and need more time to encode. This paper is just to explain how Huffman coding works.</p><p>First, In the Figure 1.1, I made a new text file. I want Python to read the strings in ASCII format. And put all of the information in the txt file to a variable: <mark class="hl-label blue">my_string</mark> . I need to get the total length of the code to do the next step.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">file = <span class="built_in">open</span>(<span class="string">&quot;data.txt&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;ascii&quot;</span>)</span><br><span class="line">my_string = file.read()</span><br><span class="line">len_my_string = <span class="built_in">len</span>(my_string)</span><br><span class="line"><span class="comment">#Figure1.1: Ask python to read the txt file named data.txt and put all information in a new string called my_string and get its length</span></span><br></pre></td></tr></table></figure><p>Because the text file contains more than 2,700 characters, I will use the short string as an example(Figure1.2) in this paper.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_string = <span class="string">&quot;ahwderghs&quot;</span></span><br><span class="line"><span class="comment">#Figure 1.2: Using this variable my_string as an example in paper</span></span><br></pre></td></tr></table></figure><h4 id="Count-letters"><a href="#Count-letters" class="headerlink" title="Count letters"></a>Count letters</h4><p>We need to find some of the unique letters. We don’t want to store lots of duplicated letters in a new array list. Also, I used a <mark class="hl-label blue">count()</mark>  built-in Python function, which allows me to count how many times they appear in my string(Figure 2.1).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">letters = []</span><br><span class="line">only_letters = []</span><br><span class="line"><span class="keyword">for</span> letter <span class="keyword">in</span> my_string:</span><br><span class="line"><span class="keyword">if</span> letter <span class="keyword">not</span> <span class="keyword">in</span> letters:</span><br><span class="line">freq = my_string.count(letter)</span><br><span class="line">letters.append(freq)</span><br><span class="line">letters.append(letter)</span><br><span class="line">only_letters.append(letter)</span><br><span class="line"><span class="comment">#Figure 2.1: A code block of checking the unique letters and their frequently</span></span><br></pre></td></tr></table></figure><h4 id="Build-leafs-and-nodes"><a href="#Build-leafs-and-nodes" class="headerlink" title="Build leafs and nodes"></a>Build leafs and nodes</h4><p>After we get all the letters count and unique letters in the arrays, we start to build the Huffman tree (Figure 3.1), where we need to have some nodes. Make sure to order the tree in ascending order after adding all of the nodes huffman_tree[] array will be used as the final tree, and the nodes array will be used to combine and merge.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">nodes = []</span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(letters) &gt; <span class="number">0</span>:</span><br><span class="line">nodes.append(letters[<span class="number">0</span>:<span class="number">2</span>])</span><br><span class="line">letters = letters[<span class="number">2</span>:]</span><br><span class="line">nodes.sort()</span><br><span class="line">huffman_tree = []</span><br><span class="line">huffman_tree.append(nodes)</span><br><span class="line"><span class="comment">#Figure 3.1: A code block of building a Huffman Tree base and their nodes</span></span><br></pre></td></tr></table></figure><p>This is a recursive function (Figure3.2) to repeat the call to build the tree from the root all the way to the last leaf. Make sure it is in ascending order before starting to build. We want to ensure the two nodes are the smallest as the Huffman algorithm is a greedy method. We need to make sure all the things are the best optimized.</p><p>Let’s take the <mark class="hl-label blue">nodes[pos]</mark>  as the left nodes and <mark class="hl-label red">nodes[pos+1]</mark>  as the right nodes. And we want to append other elements.</p><div class="note modern"><p>For example, the <mark class="hl-label blue">node[pos] = [1, ‘a’, 0]</mark>  as the left side. The <mark class="hl-label red">node[pos+1] = [2, ‘h’, 1]</mark>  is the right side. Take them as the binary codes 0 and 1.</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#this is a recursive function that combines all the letters with the frequency count for each of those letters. It will keep recurse until the length of the nodes is less than 1.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">combine</span>(<span class="params">nodes</span>):</span><br><span class="line">pos = <span class="number">0</span></span><br><span class="line">newnode = []</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(nodes) &gt; <span class="number">1</span>:</span><br><span class="line"><span class="comment">#before each iteration of the recursive process, we have to check that the nodes are arranged in ascending order.</span></span><br><span class="line">nodes.sort()</span><br><span class="line"><span class="comment">#We add 0 and 1 on each node, where the first node (located on the left side) is 0, and the second node (located on the right side) is 1.</span></span><br><span class="line">nodes[pos].append(<span class="string">&quot;0&quot;</span>)</span><br><span class="line">nodes[pos + <span class="number">1</span>].append(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"><span class="comment">#we start to combine the nodes. for example: nodes = [[1,&quot;a&quot;],[2,&quot;h&quot;]]</span></span><br><span class="line"><span class="comment">#we combine the first and the second nodes of its elements together. Their frequently count number combine together: 1 + 2 =3</span></span><br><span class="line">combined_node1 = nodes[pos][<span class="number">0</span>] + nodes[pos + <span class="number">1</span>][<span class="number">0</span>] <span class="comment"># we combine the first and second nodes of its letter, which is &quot;a&quot; and &quot;h&quot; combined to form &quot;ah.&quot;</span></span><br><span class="line">combined_node2 = nodes[pos][<span class="number">1</span>] + nodes[pos + <span class="number">1</span>][<span class="number">1</span>] <span class="comment"># We create a new node by combining these two existing nodes into a single one. We store newly created nodes in the newnode[]. newnode = [3, &quot;ah&quot;]</span></span><br><span class="line">newnode.append(combined_node1)</span><br><span class="line">newnode.append(combined_node2)</span><br><span class="line"><span class="comment">#we are now adding the new nodes together with the rest of the nodes that we haven&#x27;t used before. We should also need to remove the old nodes that were combined earlier.</span></span><br><span class="line"><span class="comment">#it will end up looking something like this: [[3, &quot;ah], [3,&quot;g&quot;], [5, &quot;s&quot;]...] The first element (node) is the newly combined node that we just completed. and the remaining ones make up the older nodes.</span></span><br><span class="line">newnodes = []</span><br><span class="line">newnodes.append(newnode)</span><br><span class="line">newnodes = newnodes + nodes[<span class="number">2</span>:]</span><br><span class="line"><span class="comment">#after that, update the nodes with the new nodes, which are [3, &quot;ah&quot;] as well as those old nodes. This is going to be used once more for the subsequent recrusive.</span></span><br><span class="line">nodes = newnodes</span><br><span class="line">huffman_tree.append(nodes)</span><br><span class="line"><span class="comment">#recursion will be called again until the nodes[] is less than 1</span></span><br><span class="line">combine(nodes) <span class="comment"># after the recursion is done, return to the huffman_tree with different nodes combined together</span></span><br><span class="line"><span class="keyword">return</span> huffman_tree</span><br><span class="line"><span class="comment">#call this recursion funciton to recurse the nodes[]</span></span><br><span class="line">newnodes = combine(nodes)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Figure 3.2: A code block of a recursive function to be called to combine nodes in Huffman Tree</span></span><br></pre></td></tr></table></figure><h4 id="Tree-built-but-need-revision"><a href="#Tree-built-but-need-revision" class="headerlink" title="Tree built but need revision"></a>Tree built but need revision</h4><p>Then we start to combine these two nodes with the least values in the huffman_tree array. So it will become something like this: <mark class="hl-label blue">newnode = [3, ‘ah’]</mark> . Then we keep combining and combining again until the length of the <mark class="hl-label blue">newnode[]</mark>  has one element left. The, we return to the final Huffman tree.</p><p><img src="/images/Algorithm/Huffman_Tree_Unreverse.png" title="Figure 3.3: Example of what the Huffman tree looks like after combing."></p><p>This is what the final tree looks like, but in Figure 3.3, it doesn’t seem correct because the tree root should be at the top where those leaves are supposed to be at the bottom. So we need another built-in python function to reverse the whole tree (Figure 3.5).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huffman_tree.sort(reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/Algorithm/Huffman_Tree_Reversed.png" title="Figure 3.5 Reversed sorting Huffman tree"></p><h4 id="Remove-Duplicated-elements"><a href="#Remove-Duplicated-elements" class="headerlink" title="Remove Duplicated elements"></a>Remove Duplicated elements</h4><p>However, we noticed lots of duplicated elements in the tree (Figure 3.5) where the Huffman tree does not accept duplicated nodes in the tree same as the Binary Search Tree algorithm. So, we need to remove those redundant elements.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">checklist = []</span><br><span class="line"><span class="keyword">for</span> level <span class="keyword">in</span> huffman_tree:</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> level:</span><br><span class="line"><span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> checklist:</span><br><span class="line">checklist.append(node)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">level.remove(node)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Figure 3.6: Remove duplicated element algorithm</span></span><br></pre></td></tr></table></figure><p>After we remove those duplicated elements by using an algorithm in Figure 3.6, we can move forward and encode those English letters into binary code. In figure 3.7, the loop will check if the targeted letter exists in each node. If the letter exists in these nodes, it will note them down; if the letter does not exist in these nodes, it will note down an empty value: “” with nothing inside the quotation marks.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#after that, we begin the process of converting these letters into our own unique binary code using the shortest amount of space possible (greedy method)</span></span><br><span class="line">letter_binary = []</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(only_letters) == <span class="number">1</span>:</span><br><span class="line">letter_code = [only_letters[<span class="number">0</span>], <span class="string">&quot;0&quot;</span>]</span><br><span class="line">letter_binary.append(letter_code \* <span class="built_in">len</span>(my_string))</span><br><span class="line"><span class="keyword">else</span>: <span class="comment"># this will check to see if the letter contains each of the nodes and then proceed accordingly. In the event that it does contain, add the binary value of the nodes, which can be found in the third position of the index. # If it does not contain anything, simply add the character sequence &quot;&quot; which is a string empty of any content and enclosed in quotation marks.</span></span><br><span class="line"><span class="keyword">for</span> letter <span class="keyword">in</span> only_letters:</span><br><span class="line">lettercode = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> checklist:</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(node) &gt; <span class="number">2</span> <span class="keyword">and</span> letter <span class="keyword">in</span> node[<span class="number">1</span>]:</span><br><span class="line">lettercode = lettercode + node[<span class="number">2</span>]</span><br><span class="line">letter_code = [letter, lettercode]</span><br><span class="line">letter_binary.append(letter_code)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Figure 3.7: Converting the letters to binary codes by using the Huffman tree.</span></span><br></pre></td></tr></table></figure><h4 id="Get-our-own-ASCII-table-and-binary-codes"><a href="#Get-our-own-ASCII-table-and-binary-codes" class="headerlink" title="Get our own ASCII table and binary codes"></a>Get our own ASCII table and binary codes</h4><p>Now we get a table with Huffman algorithm, use this array and its element to convert our English text to the binary code. Converting algorithm can be seen below:</p><div class="note modern"><p>[[‘a’, ‘1110’], [‘h’, ‘01’], [‘w’, ‘110’], [‘d’, ‘1111’], [‘e’, ‘000’], [‘r’, ‘100’], [‘g’, ‘001’], [‘s’, ‘101’]]</p></div><h4 id="Encoding-by-using-our-own-ASCII-table"><a href="#Encoding-by-using-our-own-ASCII-table" class="headerlink" title="Encoding by using our own ASCII table"></a>Encoding by using our own ASCII table</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bitstring = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">for</span> character <span class="keyword">in</span> my_string:</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> letter_binary:</span><br><span class="line"><span class="keyword">if</span> character <span class="keyword">in</span> item:</span><br><span class="line">bitstring = bitstring + item[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h4 id="Get-the-encoded-string"><a href="#Get-the-encoded-string" class="headerlink" title="Get the encoded string"></a>Get the encoded string</h4><p>Then we get a binary code with a very short length. Compared to the file size with ASCII table size is 72 bits, and we compressed it to the size of 27 bits. The size is reduced to 62% which is a very efficient algorithm to use.</p><div class="note modern"><p>111001110111100010000101101</p></div>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Articles </tag>
            
            <tag> Code </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ITS395 Final Paper Proposal</title>
      <link href="/2022/12/15/Accessibility/FinalPaper/"/>
      <url>/2022/12/15/Accessibility/FinalPaper/</url>
      
        <content type="html"><![CDATA[<h3 id="Final-Paper-Idea"><a href="#Final-Paper-Idea" class="headerlink" title="Final Paper Idea"></a>Final Paper Idea</h3><p><em>Live Sign Interpreter Anywhere</em></p><p>This is just a “pop-up” idea that I discussed in class on Thursday where I had already discussed it. It is basically a software or a plug-in that can be added to a web browser. The function is the interpreter can be toggled on and off at any time for people with disabilities who are having a negative experience or are having trouble gathering information on a computer or a television, such as watching a video or video chatting, reading a short article, or reading comments. The live time interpreter can appear in a small window in the lower-right corner of the screen and provide assistance to users at any time; alternatively, the window can be dragged to any location the user finds more convenient.</p><p><em>Closed Interpreting Accessibility Revision</em></p><p>A project in which I worked with my partner and two mentors on REU internship experiences during the summer. It is about how Sign Language Interpreters are not readily available on television or in online media. Our project offers an application programming interface (API) known as AblePlayer, and we made some modifications to this API in order to make it more supportive of features related to sign interpreters window. Considering that this project has already been completed, however, I would like to provide some revised feedbacks and make some alterations to the CIA concept. Therefore, I will eliminate some of the superfluous features, such as the transparency functions, and ensure that the sign interpreter windows do not have an excessive amount of transparency. I will also try to fix those bugs where they still existed right now.</p><p><em>Web Browser Background Color</em></p><p>The use of well-known web browsers such as Chrome, Firefox, and Edge have become an integral part of our daily lives. We rely on them on a daily basis to carry out both our productive and entertaining work. But for those individuals who suffer from a condition known as color sensitivity, which makes it uncomfortable for them to view backgrounds of black and white or a particular color. Even though some browsers provide a daytime and nighttime mode, the primary goal of this feature was originally to safeguard the eyes. So there is a tool that needs to be made public and utilized in order to assist those individuals who are color sensitive. A good idea could be to change the sensitive color set by the users, including the different website background, or a particular area for decoration, or even all images and diagrams. This would be an example of how this could be used. The sensitive color will be changed to an alternative color, but the other colors will remain unchanged.</p><h3 id="Based-on-Few-Advices-to-Made-Modifications-on-Able-Player"><a href="#Based-on-Few-Advices-to-Made-Modifications-on-Able-Player" class="headerlink" title="Based on Few Advices to Made Modifications on Able Player"></a>Based on Few Advices to Made Modifications on Able Player</h3><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>Sign Language in the media, either on-site or remotely, is provided to people who are Deaf and hard of hearing to be informed about critical information and other critical information in their daily lives. Access to Sign Language on TV or online media is not a requirement outlined within the ADA. . Deaf users are still using closed captions as their alternative options for watching TV and other media (Liu &amp; Lam, 2022). To promote quality accessibility, my previous project developed Able Player (Liu &amp; Lam, 2022), an accessible tool similar to closed captioning that will be displayed with an sign interpreter video that can be toggled on and off. This closed interpreting tool is user adjustable. It contains settings such as interpreter window size, transparency, and moveable window. All of which can be adjusted by the user. My project (Liu &amp; Lam, 2022) received numerous feedback, such as some areas of the user interface design needing to be corrected, where I need to continue to work on more about this part, and some buttons needing more detail, some developers might want to use the API, because I made a lot of modifications on the original Able Player, so a manual is required in order to provide clear instructional guidelines for using the API. This study aims to evaluate the modified Able Player tool and provide a instructional manual how to use the API for the web developer. The IRB approval is still needed where it involves human research, and I will work with more if time permits after getting approval.</p><h4 id="Introductoin"><a href="#Introductoin" class="headerlink" title="Introductoin"></a>Introductoin</h4><p>The utilization of accessibility has grown as a result of the fast development of high-speed transmission and video technology. Deaf and hard of hearing people are increasingly adapting to acquire information from television shows, films and websites. Thus, accessibility such as captions both in closed captions and open captions are become very important where deaf and hard of hearing people are rely on them as a very valuable tool for translating spoken language. (Liu &amp; Lam, 2022). However, the captions as the major accessibility are still an “ad-hoc” for deaf and hard of hearing. One example is the YouTube Video where it was designed to those who speak, read and write English very well and it is only use as the alternative option to replace aural information to many deaf viewers and they find it very difficult to follow because the speed of verbatim captioning is likely to exceed their reading abilities . Even after controlling for reading level, deaf students learned less from on-screen text than hearing peers. They prefer ASL interpreters over closed captions, partially because they find captions difficult to understand and partly because they receive less information (Debevc et al., 2015).</p><p>I had another option where it provided the PIP (Picture in Picture)mode with the sign language interpreter on TV and internet vides. However, the media tends to crop out the screen of the interpreter if it is even available to all, which prevents the DHH viewer from getting full access to information. Providing PIP on a few channels or sources achieves critical mass, albeit with limited success. If not properly set up, it might be difficult for the deaf audience to see what is being spoken, resulting in incomplete communication. It only caters to the DHH community on a limited level, therefore this issue may be difficult for DHH individuals because the media is hearing-centric, and they have little experience with sign language translating on TV (Liu &amp; Lam, 2022).</p><h4 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h4><p>The research will be split into two parts: one will concentrate on the user experience and user interface, while the other will be geared toward website developers. Both the user and the viewer will have a better experience watching content that is more accessible if a good instruction on how to utilize the Able Player API for website developers is provided. The steps outlined below are broken up into two parts. The first step is to write a manual, it will be referred to by the person responsible for developing the website in order to correctly configure and install the Able Player there. The second option is a modified version of Able Player, which has had bugs corrected and unnecessary functionalities removed allow user to watch without any problem.</p><p><em>Able Player Manual:</em><br>I created and participated on a project that offered a more accessible media player, called Able Player, and allowed users to use it whenever they wanted. However, there are still some problems with this project, so I made some improvements to the accessible media player to make it more user-friendly and accessible for everyone after the survey and Q&amp;A interview session on that project. Additionally, I want to ensure that the web developers can use this encapsulated API to set up all of the functionality without any issues. Therefore, a detailed manual is necessary to utilize in order to expand on how to use the Able Player as needed.<br>The manual will be included:</p><ul><li>General instructions</li><li>Purpose of making this modification</li><li>The reference documents</li><li>API environment requirements</li><li>Assistance and problem reporting</li><li>Common issues where developer encounters</li><li>License</li></ul><p><em>Survey:</em><br>I intend to get in touch with the previous participants who volunteered their time and see if they would be interested in coming back to try out my improved version of the Able Player. It is my expectation that there will be approximately two to three persons there, and that number will be sufficient for me. If there is enough time, I could ask one or two website developers to participate in this research study as new participants and then show them my API along with the manual that I just finished writing for the project. The survey will be use in Google Form and will be analyzed after the usability test done. It will include five multiple-choice questions, and respondents will be asked to rank each question on a scale from 1 to 5 based on the degree to which they agree with the statement that they are reading. Also, there will be a open-discussion Q&amp;A that ask participants for their feedback regarding this project and the modification that I had been made.</p><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p>It is intended that I will spend around ten minutes with each participant, explaining the various informed consent forms either in American Sign Language (ASL) or spoken English. They are going to be provided with a general explanation of the goal of this research.</p><p>I will supply each participant with a laptop, and they will all meet in a room individually. They are free to bring own computer. However, their computer must be capable of running Chrome Version 109.0.5xx or a later version, as well as Microsoft Edge Version 95.x or the most recent version. I will supply the pre-set Able Player with the videos that were used in the previous project, along with the American Sign Language interpreter. At this time, I will make it shorter by only requiring one video to watch from each participant, and the topic will be the same throughout. The video #3: Gas Price in Los Angeles from the previous project will be use to all participants in Figure 1:</p><p><img src="/images/REU_CIA/CIA,figure2.png" title="Figure 1: Example of an Early Able Player Media Version"></p><p>Because I was not funded by any organization or funding support, none of the participants will receive any compensation for their time spent participating in the usability study. The replies of all participants in both the usability study and the survey were kept strictly confidential and were only meant to be discussed between the researcher I am working on this project.</p><h4 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h4><p>The approval of the IRB is a significant obstacle given that this study includes variables of human research. For now, I should wait to get the IRB approval to move forward with the evaluation and research test. From the Able Player itself in the survey from my previous project, a number of the participants reported that they had a favorable opinion of the media player use. We conducted our research with a total of 10 participants, and we utilized the same survey for each round of usability testing. In order to assess and present the results, we used an Excel chart generating tool to depict each participant’s responses. The scores for individual participants and different hearing status groups of participants were examined shown in Figure 2, and the scores for most participants were medium-high level which is considering a great and useful media player to them.</p><p><img src="/images/REU_CIA/CIA,figure3.1.png" title="Figure 2: A Chart of the System Usability Scale Calculation by Score"></p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Because the modified version of Able Player in version 1.0 received a great deal more positive feedback, I do expect to have a successful outcome with the modified version of Able Player. Also, I am unsure whether the web developer will find the handbook to be understandable or not. Therefore, I still require further time to figure out.</p><h4 id="The-Innovation-of-Speech-English-to-Auto-Generating-Sign-Language-Interpreting"><a href="#The-Innovation-of-Speech-English-to-Auto-Generating-Sign-Language-Interpreting" class="headerlink" title="The Innovation of Speech English to Auto Generating Sign Language Interpreting"></a>The Innovation of Speech English to Auto Generating Sign Language Interpreting</h4><p>All sign interpreting is pre-recorded and prepared for usage in the Able Player or other media source in accordance with the WCAG 2.1 Guideline. However, where the WCAG does not include it with the auto-generating sign interpreting similar to the auto-generating captions.</p><p>When you want to make accessibility features for the DHH community, it is a big problem because you have to hire certified sign language interpreters and have video editing skills to ensure that the DHH audience has the best possible watching experience. If more videos are provided about the website’s development, the budget might increase to a significant amount. The captions had the exact same problem in the very beginning stage, but thanks to improvements in closed captioning with more advanced technology, there are now captions that are automatically generated and support a variety of languages. It works with speech recognition technology, which enables the computer to listen like a human, generate captions, and display captions on screen.</p><p>The future will have high accuracy AI technology that can do anything a computer can. The idea is the same with sign interpreting as well because it serves the same purpose of translating speech English into another language. For instance, the official Chinese Central Television (CCTV) successfully used AI interpreting in 2021 in conjunction with Chinese sign language recognition with different recognition algorithm (Gao et al., 2022). I believe the future will use this concept of AI technology to create better DHH accessibility and the least amount of work possible for website developer without any additional process.</p><h4 id="Future-Works"><a href="#Future-Works" class="headerlink" title="Future Works"></a>Future Works</h4><p>There were technological concerns that necessitated the assistance of additional UI and UX professional consultants and specialists, such as incorporating a new design style on a dropdown menu. A professional can provide far more beneficial recommendations. While working on this enormous project since 2014, more than three or four individuals are also encouraged to contribute to the future development of Able Player. It is my expectation and encouragement that more people will use this Able Player tool to increase accessibility because it is a wonderful tool to use as needed requiring less work and develop time and also followed with the WCAG. I hope that more people will use this tool, and I will encourage them to do so.</p><p>Another significant problem that my project team ran into was the enormous amount of time required to work on the sign interpreting videos. I am expecting to wait for the concept of auto generating AI sign interpreting where they can release the open-source API that website developer can use it with support for American sign language. The amount of work can be significantly decreased.</p><h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><table>    <tr>        <td>Bosch-Baliarda, M., Soler-Vilageliu, O., & Orero, P. (2020). Sign language interpreting on TV: A reception study of visual screen exploration in deaf signing users. MonTI. Monografías De Traducción e Interpretación, (12), 108–143. https://doi.org/10.6035/monti.2020.12.04         </td>    </tr>        <tr>        <td>Brooke, J. (1986). “SUS: a “quick and dirty” usability scale”. In P. W. Jordan, B. Thomas, B. A. Weerdmeester, & A. L. McClelland (eds.). Usability Evaluation in Industry. London: Taylor and Francis.        </td>    </tr>        </tr>        <tr>        <td>Cronin, B. J. (2013, April 22). Chapter 14: Closed-caption television: Today and Tomorrow. American Annals of the Deaf. Retrieved June 7, 2022, from https://muse.jhu.edu/article/386799/pdf        </td>    </tr>        </tr>        <tr>        <td>Debevc, M., Milošević, D., & Kožuh, I. (2015). A comparison of comprehension processes in sign language interpreter videos with or without captions. PLOS ONE, 10(5). https://doi.org/10.1371/journal.pone.0127577        </td>    </tr>        </tr>        </tr>        <tr>        <td>Gao, W., Chen, Y., Zhao, D., & Fang, G. (2022). A chinese sign language recognition system based on SOFM/SRN/HMM. Pattern Recognition. Retrieved December 11, 2022, from https://www.sciencedirect.com/science/article/abs/pii/S0031320304001657        </td>    </tr>        </tr>        </tr>        <tr>        <td>Huang, C.-wei. (2003). Automatic Closed Caption Alignment Based on Speech Recognition Transcript. CiteSeerX. Retrieved June 7, 2022, from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.233.419        </td>    </tr>        </tr>        </tr>        <tr>        <td>Liu, Z., & Lam, B. (2022, July 28). Closed Interpreting Accessibility. Google Doc. Retrieved December 6, 2022, from https://docs.google.com/document/d/11iwfn-NgzxBQlD4kN6lwVBSclLsJ_B6Y1WJBZ0hNKTE/edit        </td>    </tr>        </tr>        </tr>        <tr>        <td>Yi, J. H., et al. (2021). Design Proposal for Sign Language Services in TV Broadcasting from the Perspective of People Who Are Deaf or Hard of Hearing. Applied Sciences, 11(23), 11211. https://doi.org/10.3390/app112311211        </td>    </tr></table>]]></content>
      
      
      <categories>
          
          <category> Accessibility Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Accessibility </tag>
            
            <tag> Articles </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ITS395 Final Exam</title>
      <link href="/2022/12/14/Accessibility/FinalExam/"/>
      <url>/2022/12/14/Accessibility/FinalExam/</url>
      
        <content type="html"><![CDATA[<ol><li><p>What is universal design, according to the Nielsen Norman Group?</p><ul><li><mark class="hl-label blue">create one experience that can be accessed and used to the greatest extent possible by all people.</mark> </li><li>create multiple experiences that can be accessed and used to the greatest extent possible by all people.</li><li>modify the experiences so that theycan be accessed and used to the greatest extent possible by all people.</li><li>provide a single experience that is accessible by all people.</li></ul></li><li><p>What is inclusive design, according to the Nielsen Norman Group?</p><ul><li>methodologies to create products that understand and enable people with diverse sensory abilities.</li><li>methodologies to create products that understand and enable people with diverse motor abilities.</li><li><mark class="hl-label blue">methodologies to create products that understand and enable people of all backgrounds and abilities.</mark> </li><li>methodologies to design products that serve customers.</li></ul></li><li><p>According to w3c.org, web accessibility means:</p><ul><li>that websites, tools, and technologies are designed and developed to the greatest extent possible by all people.</li><li><mark class="hl-label blue">that websites, tools, and technologies are designed and developed so that people with disabilities can use them.</mark> </li><li>that websites can be viewed by anyone.</li><li>that websites, tools, and technologies are designed and developed so that intended customers can use them.</li></ul></li><li><p>WCAG’s POUR guidelines state that accessible web content must be:</p><ul><li>Possible, operable, understandable, and real.</li><li>Possible, operable, understandable, and robust.</li><li><mark class="hl-label blue">Perceivable, operable, understandable, and robust.</mark> </li><li>Personal, operable, understandable, and real.</li></ul></li><li><p>In the “A List Apart” article, how many people are completely color-blind?</p><ul><li>1 in 100</li><li>1 in 1,000</li><li>1 in 3,000</li><li><mark class="hl-label blue">1 in 30,000</mark> </li></ul></li><li><p>How can a person who is color blind tell traffic lights apart?</p><ul><li>by color</li><li><mark class="hl-label blue">by position</mark> </li><li>by brightness</li><li>by contrast</li></ul></li><li><p>How can we make an image accessible to someone who can’t see it?</p><ul><li><mark class="hl-label blue">Add alternative (“alt”) text that describes the image to screen readers.</mark> </li><li>Create a separate version of the page with no images.</li><li>Pictures and photos are visual and should not be used under any circumstances.</li></ul></li><li><p>Who benefits from captions and transcripts for audio content?</p><ul><li>Deaf or hard of hearing people.</li><li>People in a noisy or quiet environment.</li><li>People with cognitive disabilities who benefit from reading text on screen while listening to it.</li><li><mark class="hl-label blue">All of the above.</mark> </li></ul></li><li><p>Which of the following is recommended for color accessibility?</p><ul><li><mark class="hl-label blue">Ensure a high level of contrast between the color of text and the background color.</mark> </li><li>Create a separate black-and-white version of the page.</li><li>Vibrant, rich color combinations should be strictly avoided, no matter what.</li><li>All of the above</li></ul></li><li><p>Certain disabilities make it difficult to understand figurative language. Providing assistance is vital for these audiences. It’s recommended with usage of an unusual or misunderstood word, to:</p><ul><li>Do nothing. The user can look up a word he/she doesn’t know</li><li><mark class="hl-label blue">Link the first instance of the word to the definition on a Web page</mark> </li><li>Define the word inline within the sentence</li><li>None of the Above</li></ul></li><li><p>Web accessibility focuses on making content accessible to</p><ul><li>People with diverse movement abilities.</li><li>People with diverse sensory abilities</li><li>People with diverse cognitive abilities</li><li><mark class="hl-label blue">All of the above</mark> </li></ul></li><li><p>Who can provide valuable feedback in order to make the website more accessible?</p><ul><li>Elderly people.</li><li>People with disabilities</li><li>Children</li><li><mark class="hl-label blue">All of the above</mark> </li></ul></li><li><p>According to the “Accessible Images For When They Matter Most” article, what is the percentage of visual learners?</p><ul><li>25%</li><li>50%</li><li><mark class="hl-label blue">65%</mark> </li><li>90%</li></ul></li><li><p>According to the “Article - Paint the Picture, Not the Frame: How Browsers Provide Everything Users Need”, users who are told that they were rejected by the system rather than another human are more likely to be</p><ul><li><mark class="hl-label blue">angry</mark> </li><li>happy</li><li>sad</li><li><mark class="hl-label blue">morose</mark> </li></ul></li><li><p>In the book “Web Accessibility : Web Standards and Regulatory Compliance”, the purpose of alt text is fully accomplished when:</p><ul><li>the page is more useful with images</li><li>the page is more useful with alt text</li><li><mark class="hl-label blue">the page is equally useful with images or alt text</mark> </li><li>the page is accessible to everyone with images</li></ul></li><li><p>In the book “Web Accessibility : Web Standards and Regulatory Compliance”, what accessible technology feature can help users with multiple sclerosis?</p><ul><li><mark class="hl-label blue">sclerotic feature</mark> </li><li>mouth stick</li><li>keyboard</li><li>voice</li></ul></li><li><p>What is the full name for PDF?</p><ul><li>Printable Document Format</li><li><mark class="hl-label blue">Portable Document Format</mark> </li><li>Picture Document Format</li><li>Pretty Document Format</li></ul></li><li><p>In the article: “Accessible Images For When They Matter Most”, how do you mark decorative images using alt img?</p><ul><li>alt img=”alt”</li><li><mark class="hl-label blue">alt img=</mark> </li><li>alt img=”SSN”</li><li>alt /</li></ul></li><li><p>People with dyslexia benefit from:</p><ul><li>alt text</li><li>alt img</li><li>kerning</li><li><mark class="hl-label blue">typography</mark> </li></ul></li><li><p>Creating accessible images requires more than just adding alt text.</p><ul><li><mark class="hl-label blue">True</mark> </li><li>False</li></ul></li><li><p><b>Describe at least four differences between direct touch screen interaction and interaction in using VoiceOver or TalkBack on iOS or Android devices.</b></p><p>Direct touch screen interaction and interaction on iOS or Android devices with VoiceOver or TalkBack differ in a number of ways. Among the most significant variations are:</p><ul><li>Feedback: Direct touch screen interaction is an input method where users enter commands by touching the screen with their fingers. Users can enter commands by speaking aloud or using gestures with VoiceOver or TalkBack.</li><li>Direct touch screen interaction provides users with visual feedback that they can see by staring at the screen. Users who use VoiceOver or TalkBack hear spoken words or other sounds as auditory feedback.</li><li>Navigation: With direct touch screen interaction, users swipe or tap the screen to move around the user interface. Users can use voice commands or gestures to navigate when using VoiceOver or TalkBack.</li><li>Accessibility: Users with specific disabilities, such as those who are blind or have low vision, may find direct touch screen interaction inaccessible. On the other hand, VoiceOver and TalkBack are specifically made for people who have vision problems, and they offer accessibility features like text-to-speech, screen magnification, and braille support.</li></ul><p>Overall, the input method, feedback, navigation, and accessibility are the main differences between direct touch screen interaction and interaction using VoiceOver or TalkBack on iOS or Android devices. These variations may have an impact on the user experience and may necessitate that users change how they interact with the device in order to make the most of it.</p></li><li><p><b>Describe at least four problems that may arise in using Youtube’s automatic speech recognition feature.</b></p><p>When using YouTube’s automatic speech recognition (ASR) feature, a number of issues could occur. These issues might make it difficult for users to use the ASR feature effectively and may affect the accuracy and value of the transcriptions. Among the crucial issues are:</p><ul><li>Accurate transcription: Using ASR technology presents a number of challenges, including the need for high accuracy in the transcription of spoken words. ASR systems occasionally make transcription errors, which can lead to inaccurate or deceptive transcriptions.</li><li>Background noise: ASR can have trouble accurately transcribing speech in the presence of background noise, which is another issue with the technology. For instance, the ASR system might struggle to accurately transcribe the speech if there is music or other background noise in a video.</li><li>Different accents and languages: Because ASR systems are typically trained on a particular accent and language, they may struggle to accurately transcribe speech in these situations. Users with different accents or who speak different languages may find it difficult to use the ASR feature as a result, which can result in inaccurate transcriptions.</li><li>Limited functionality: ASR systems are primarily used to transcribe speech and do not typically offer any other features. For instance, speaker identification or other contextual information cannot be automatically generated by ASR systems in captions. As a result, the transcriptions may be less useful and may be more challenging for users to comprehend and utilize effectively.</li></ul></li><li><p><b>Explain what is ARIA – and give at least two examples.</b></p><p>The Accessible Rich Internet Applications Suite (ARIA), is a set of technical guidelines that specify how to make web content and web applications more accessible to people with disabilities. Web developers can use other web accessibility standards like HTML and CSS to create more inclusive and user-friendly web experiences by utilizing ARIA attributes and roles.</p><p>The following are some instances of ARIA attributes and roles:</p><ul><li>Using the “aria-label” attribute, you can give a text label to a web page element like a button or form field. This can aid users who are blind in comprehending the function of the element and interacting with it appropriately.</li><li>Use the “role” attribute to specify an element’s function or purpose on a web page. An element’s “role” attribute, for instance, can be used to specify whether it’s a form field, a button, or a navigation menu. This can make it easier for users with disabilities to interact with the website and understand its organization and structure.</li></ul></li><li><p><b>Give four improvements in WCAG 3.0 over 2.x, per the article, “The Future of Web Accessibility”.</b></p><p>Some of the features from WCAG2.1 will be carried over to the new standard, but there will also be some new features. But since the standard has changed from A, AA, and AAA to a number score. It is the number that goes from the lowest possible score on the standard to the highest possible score. Its purpose is to show how important and necessary it is to measure accessibility. It is more flexible, which made it easier for both users and developers to understand how to measure the accessibility standard. It also made the product easier to use.</p><p>Because the WCAG2.x standard is hard to fully understand, the readability will be changed in a big way. So, the members are thinking about adding symbols and letters that are easier to understand and make more sense so that people who are reading the WCAG for the first time can understand it quickly. It is possible, among other things, to get rid of some unnecessary contexts and rebuild the category class.</p><p>The standard has also been made stricter, such that if one fatal had been made, even if it’s just one fatal mistake, the corresponding standard will drop down to a score of zero, and it will be marked as inaccessible.<br>It adds a new standard known as conformance, which rates websites as Bronze, Silver, or Gold according to the WCAG3.0 Draft. The WCAG3.0 minimum requirement for conformance is called bronze, and the best conformance level is called gold. Bronze is the lowest level of conformance that can be achieved.</p></li><li><p><b>Per the article, “Accessible Images For When They Matter Most”, explain why it is appropriate for some images to be labeled “Decorative Images” and therefore does not need to be described.</b></p><p>Some images should be called “decorative images” and not be described because they don’t give the user any important information or functionality. Most of the time, decorative images are used for their looks, like to add visual interest or improve the look of a web page. They don’t help the user understand or interact with the content. So, it’s not necessary to explain what these images are because they don’t do anything useful and aren’t important to the user’s experience.</p><p>Labeling an image as “decorative” can help people with disabilities who use tools like screen readers to get to web content. When a screen reader comes across an image that has been marked as decorative, it can be set to ignore it and not try to describe it. This can help the user see less information that isn’t important or isn’t relevant and make it easier for them to focus on the most important information. So, calling images “decorative” is the right thing to do when they don’t give the user any important information or functionality. This can help make web content more accessible and easier to use for people with disabilities.</p></li><li><p><b>In the article, “Keyboard-only” Navigation for Improved Accessibility, explain why they recommend using a “Skip Navigation Link”.</b></p><p>Putting a “skip navigation link” on a web page is a good idea because it can make the page easier for people with disabilities to access and use. A skip navigation link is a link that lets users skip a web page’s main navigation menu and go straight to the main content. This can be especially helpful for users who need help getting to web content, like those who use screen readers.</p><p>Screen readers usually read web content in a straight line, from the top of the page to the bottom. This means that people who use screen readers must hear the whole navigation menu before they can get to the page’s main content. This can take time and be annoying, especially if the user knows the site well and already knows where they want to go. Users can skip the navigation menu and go straight to the content they want to see with a “skip navigation” link. This makes it easier and faster for them to find the information they need.</p><p>Overall usability and access for people with disabilities of a website by giving users a way to get around the main navigation menu, “skip navigation” links can make it easier and faster for them to get to the content they want. They can also make the user experience better and less frustrating. These links can be especially helpful for people who use assistive technology like screen readers or who have trouble navigating the web with a mouse or other pointing device. Because of this, skip navigation links are often recommended as a best practice for making web content more accessible.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Accessibility Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Exams </tag>
            
            <tag> Accessibility </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Graduate School/Job Application</title>
      <link href="/2022/12/14/STM403/GraduateSchoolApplication/"/>
      <url>/2022/12/14/STM403/GraduateSchoolApplication/</url>
      
        <content type="html"><![CDATA[<h3 id="Cover-Letter"><a href="#Cover-Letter" class="headerlink" title="Cover Letter"></a>Cover Letter</h3><p>Interviewer Name<br>800 Florida Ave NE<br>Washington, D.C 20002<br>202-456-7890</p><p>Dear xxx,</p><p>I’m excited to apply for the intern technician position posted on Indeed.com as a semester term intern of the work done by the team at REU AICT. As a BS in Information Technology student at Gallaudet University, I’m confident that my diverse knowledge, experience in coding and problem solving, and meticulous attention to detail will make me an asset to the intern and class/personal project teamwork.</p><p>I assisted several more students in classes and on social media platforms such as StackoverFlow and Github. I was in charge of troubleshooting a wide range of technical issues for others, assisting with unexpected system errors, and installing a wide range of equipment. I was also constantly looking for new ways to learn new skills and gain experience by participating in various activities with people from all walks of life. With these kinds of experiences, I was able to adapt my approach to various problems and situations from various backgrounds, as well as their critical-thinking and ideas that differed from mine.</p><p>I also understand that [Company name] is looking for more interns and experienced employees to help with their production needs. The position of IT technician at [Company’s name] would be an exciting opportunity for me to apply my educational background and learn more about a growing industry.</p><p>I’ve attached my resume, which goes over my skills and educational background in greater detail. If you have any questions about my background and experiences, don’t be hesitate to contact me. Thank you very much for your consideration and time.</p><p>Sincerely.<br>Zehui Liu</p><h3 id="Job-Listing-and-Background-Research"><a href="#Job-Listing-and-Background-Research" class="headerlink" title="Job Listing and Background Research"></a>Job Listing and Background Research</h3><p>Interesting Job Position:</p><ol><li>Summer Technology Internship -Cooperate by GEICO at Chevy Chase, MD:</li><li>IT Systems Specialist Central Office located at Montgomery County Public Schools at Rockville, MD</li><li>Entry Level Developers locate at Mclean, VA of Diamond Pick Company</li><li>US Army Information Technology Specialist (FT) by Goarmy.com Spring Recruiting Center located at Silver Spring, MD</li><li>Entry Level Amazon Connect Engineer by Amazon at Bethesda, MD</li><li>Technical Analyst Internship/Entry Level by CGI Group, INC located at Fairfax, VA</li><li>Operational Compputer Systems Analyst (System Administrator) by National Security Agent located at Fort Meade, MD</li><li>IT Help Desk Analyst by Northern Virginia Community College located at Fairfax County, VA</li><li>Cyber Security Engineer, Enter Level by CGI Group, Inc located at Fairfax, VA</li><li>10 week Intern Program by USPS, work cross-functionally on projects, remotely.</li></ol><p>Summer Technology Internship -Cooperate by GEICO at Chevy Chase, MD:<br>GEICO is an insurance company, and its most popular insurance is auto insurance. With excellent customer service, GEICO has become one of the most popular and favorite insurance companies in the United States, as well as mine. I’ve had a lot of help from the agent with my vehicle issues over the last three years, So I’d like to take advantage of this opportunity to gain experience with their business management system and see how they could use it to serve people so that I can understand work as an entry level or internship developer.<br>A 10-week term internship during the summer of 2023, with the opportunity to take a quick look and view their professional technology solutions management by specialists mentoring and coaching. Individual and team assignments provide an excellent opportunity to develop and gain business-standard technical and project management experience, as well as communication skills. The qualifications are slightly higher than for other intern positions. It did require a cumulative GPA of at least 3.5, as well as the upload of my transcript for verification. Following the coursework, a fundamental understanding of some of the languages such as Java, .NET C#, or related work from a personal/past experience project is required.</p><h3 id="Resume"><a href="#Resume" class="headerlink" title="Resume"></a>Resume</h3><p>/&gt;</p>]]></content>
      
      
      <categories>
          
          <category> STM403 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Articles </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>University Mobile Exercise App and Leisure Time Exercise Activity, A Light-Weight Development of the App Concept Design and Evaluation</title>
      <link href="/2022/12/14/STM403/FinalPaper/"/>
      <url>/2022/12/14/STM403/FinalPaper/</url>
      
        <content type="html"><![CDATA[<p>To the STM403 Managers:<br>We are delighted that our project was chosen, and we are honored to have the chance to discuss the concept of our initiative here. It is evident that you are looking for an enthusiastic group team that can be depended upon to completely engage with the role and develop professionally in a self-motivated manner. We are excited to bring this project to a close since it has the potential to have a tremendous influence not only on the Gallaudet University campus but also on the campuses of other universities.</p><p>This initiative was conceived to enhance and motivate students, as well as bring them into a brighter future in terms of their education in general and their physical education in particular. We understand our student urgently needs a solution for an application. That’s why our STM group is attempting to develop and improve a comprehensive approach to app development that takes guessing out of the game. We’re ecstatic that you’re considering working as a partner with us. The following proposition will outline a strategy for proceeding with the project from the beginning to the conclusion. We are confident that we will completely understand the process and the anticipated amount of time needed to complete the task. I am sure you will acknowledge the information we gave and the high quality of the standards.<br>If you have any questions regarding our project you’d like to discuss, we would be more than happy to meet with you for clarification.<br>Sincerely,<br>Abraham, Charles, Obaidullah, and Zehui</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Numerous studies reveal a marked drop in physical activity participation and increased sedentary behavior among young adults during the college years (Calestine et al.,.2017). Due to a lack of fitness, health education, and workout equipment, more than half of university students in the United States do not engage in enough physical activity to improve their health, despite being motivated to lose weight or gain muscle (Wharton et al., 2008). Therefore, the appropriate interventions and guidelines are needed to measure and support student’s physical activity. Our project aims to promote quality fitness for our Gallaudet University students. We set aside some time to investigate the drawbacks of Gallaudet fitness and determine the areas that could most benefit from improvement. We used the ACSM (The American College of Sports Medicine) as the reference to coming up with an early development mobile app that allows our users to use the guideline with a simplified approach effectively. This project aims to understand our users’ opinions about fitness in university physical fitness centers and mobile fitness apps. We tried to collect some feedback on the performance of our newly developed fitness application in terms of several variables, including interface designs, workout instructions, ease of use, and accessibility. A group of participants will be given access to our application. We collect feedback from them to see if our application design is practical and capable from their point of view. We anticipate getting additional findings, at which point we can continue providing analysis, whether critical or complimentary, on many fitness-related subjects. It is possible that we will not be able to offer the final results due to unforeseen reasons, including the delay of the IRB approval. But what can be seen is that there are numerous trending and well-liked fitness applications, particularly during and after the COVID-19 pandemic (Cugusi et al., 2021), so we expected the mobile fitness app that including ACSM guidelines for fitness instruction, which would be advantageous to all users and worthy of continued development by future handovers.</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><em>Background:</em><br>Daily physical activity, including mobile phone apps related to physical activity and exercise, provides various health benefits and is one of the recommendations for addressing lifestyle-related diseases in the United States (Guo et al., 2022). Regular physical activity can reduce the risk of many diseases and even death. It can also lower the risk of mental health problems such as depression, anxiety, or other mental diseases. With the speedy development of mobile technologies, mobile apps related to physical activity have a great potential to improve participation rates in exercise programs (Gowin et al., 2015, Grave, 2020).<br>Exercise guidelines are set forth by the American College of Sports Medicine (ACSM). ACSM recommends that the frequency, intensity, time, and type (FITT) principle be followed for any exercise program to have health benefits and avoid injuries (Thompson, 2013, Guo et al., 2022). The ACSM principles of exercise prescription recommend three main components. These principal components are aerobic exercise, strength and resistance, and flexibility. These components play a significant role in improving health. In fact, no apps scored inadequately against the ACSM exercise prescription guidelines (Guo et al., 2022).<br>Furthermore, nutrients also play a substantial role in mental and physical health. Some studies showed that diet and nutrition are essential for physiology and body composition. Not only that, but they also significantly affect mood and mental well-being (Muscaritoli, 2021). According to the National Institute of Health, several data have found a relationship between nutrition, physical fitness, and mental health. Nutrients and physical activity are the agents for prevention, treatment, or augmentation of treatment for mental disorders in children, adolescents, and adults (Grave, 2020, Muscaritoli, 2021).</p><p><em>Mobile Application Background:</em><br>A program that carries out specific tasks is an application or app. It is now an essential component of our life. Apps can be accessed on various mobile computing platforms, making using and accessing them simple. The apps were initially intended to be used for mail, contacts, calendars, etc., but due to increased public demand, they were expanded to include mobile games, GPS services, recharge, and ticket booking options, and fitness services, resulting in millions of apps that are now available on different application distribution platforms. (Hoon et al,. 2013). Apps can assist with setting fitness objectives, offering exercise suggestions, monitoring calorie intake, offering workout recipes, and offering progress graphs. Using the data one provides, apps also offer tailored advice. Numerous factors, including increased privacy, accessibility, time constraints, and lower costs compared to fitness centers, have contributed to the growth of app usage (Tong, H.L. et al. 2022).<br>An application can improve accessibility for individuals who are deaf or hard of hearing. Many mobile apps rely on audio instructions and communication, which can be difficult for deaf individuals to access and understand. By incorporating ASL interpreters, a mobile app can provide visual, rather than auditory, instructions and communication, making it much more accessible and user-friendly for deaf individuals. Another benefit of a mobile app with ASL interpreters is that it can help to promote inclusivity and accessibility in various industries and situations. Deaf individuals may often feel excluded or disadvantaged when accessing information and services due to the reliance on auditory communication. The data shows that the ASL interpreters scored 3.72 out of 5, which is the second-highest accessibility use in mobile health (mHealth) apps (Romero* et al., 2019). By providing ASL interpreters, a mobile app can help break down these barriers by providing ASL interpreters and making information and services more inclusive for deaf individuals.</p><h3 id="Issues"><a href="#Issues" class="headerlink" title="Issues"></a>Issues</h3><p>There is no current app for Gallaudet athletes and non-athlete students to access ASL information related to activity, training, and nutrition. There is no platform for visual workout lesson plans with ASL accessible where coaches can create their plans and assign the team to follow the process. Non-athlete students have limited access to the Gallaudet fitness center due to a lack of hours. They have less knowledge of fitness and nutrition due to a lack of a fitness coach or guide leader on how to exercise and food properly. We want to ensure that Gallaudet Community has access to fitness and nutrition information to reduce physical and mental health risks.</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>In today’s busy world, many people struggle to find the time and motivation to maintain a regular fitness routine. A mobile fitness app can provide users with a convenient way to track their progress and see their improvements over time. The app can also provide reminders and notifications to help users stay on track with their fitness goals. A fitness app can also provide users with a wide range of fitness-related information and resources. This can include exercise demonstrations, workout plans, nutrition guides, and other helpful tools. By providing users with all of the information and resources they need to achieve their fitness goals, a fitness app can be a valuable tool for improving overall health and wellness (Gowin et al., 2014). Another important purpose of a mobile fitness app is to help users connect with others who have similar fitness goals. One research shows that sharing workout progress and data with others can be an excellent motivator for many people to continue exercising where it provides a sense of accountability (Ehrlén, 2020). Many apps include features that allow users to share their progress and motivate each other, creating a sense of community and support. This can be especially helpful for individuals who are new to fitness or who may be struggling to stay motivated. Overall, there will be many benefits, and the mobile fitness app can provide individuals with a convenient and accessible way to track and improve their physical fitness. These apps can help users achieve their fitness goals and improve their overall health and wellness by offering a wide range of features and resources.</p><h3 id="Objectives-Main-Goals"><a href="#Objectives-Main-Goals" class="headerlink" title="Objectives/Main Goals"></a>Objectives/Main Goals</h3><ul><li>To understand the reason why Gallaudet fitness is lacking services such as coaches and equipment</li><li>To understand whether fitness applications are preferred over fitness centers</li><li>To understand user perspectives about fitness apps</li><li>To understand the popularity of fitness apps among users</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><em>Survey Setup:</em><br>We plan to set up a questionnaire survey at the early testing stage to collect data. To obtain valuable high-level responses and in-depth insights via an interview with each participant at the end of the testing stage will be conducted. This step is to get additional clarification information, answer participants’ confusing questions, and receive feedback on our project idea. Participants will rank each question from 1 to 5 based on how much they agree with the statement they are reading. There are English and ASL versions available for use. The surveys are done through Google form. Due to waiting for IRB approval, we decided to discuss what features and tools will be included in the app design.</p><p><em>Mobile Application Setup:</em><br>To help users understand what they should do about fitness, we integrated our ACSM guidelines into the mobile app. The interface, created with Adobe XD, included information on the workouts, exercises, and dietary intake. Since our applications are still in the early stages of development, only the volunteer participants were permitted to test the app. Fitness apps come in a few main functions to serve the user’s diverse needs:</p><ul><li>Workout Plan - provide users with a goal plan such as a fit body plan, lose weight plan, etc. To help users reach their goals.</li><li>Activity Tracker — Uses mobile sensors to track a person’s daily motions, such as walking, jogging, and running, and then analyzes them for the user.</li><li>Personal Trainer- helping those who do not want to attend fitness classes and those with busy work schedules. The app acts as a fitness coach and lets users create their trains.</li><li>Diet and Nutrition- offering the options best suited to those seeking a balanced diet and who are health-conscious.</li><li>Stretch/workout daily information - helping users to understand what benefits for their physical or mental from stretch/workout</li></ul><p><em>MobileApplication Concept/Design:</em><br>We have three main structures (Figure 1) to make up our application: Home, Plans, and Daily. Each section has features, such as the home section’s recommendation of a workout plan to attract users; the General section is the shortcut for quick start exercise, monitoring activity, and checking gym schedule. It has the personalized workout plan or the suggested plan we offered in the Plans section. The information about meal suggestions and nutrition has been gathered for the Daily section. Additionally, it has an activity tracker that measures the user’s heart rate while exercising and the number of steps you take while running or walking. not just that only, but it also has a nutrient activity tracker with accurate information. This process is the same concept as dietary analysis with MyFitnessPal that has very accurate information with nutrient activity tracker. (Evenepoe et al., 2020).</p><p>hover to see the title text:</p><p>Inline-style:</p><!-- <img src="/images/stm403,figure1.png" alt="Italian Trulli"> --><p><img src="/images/stm403,figure1.PNG" title="Figure 1: The Flowchart Showing the Mobile Application Structure"></p><p>When a user clicks on the exercise video before beginning, the application provides the prerequisites and instructions outlined by ACSM, which are also shown in the exercise video. When describing what should be done and what should not be done, as well as what can be expected after continuing to work out the plan after a week or a month, it must be very specific. These videos embed the various exercises with sign language instructions, and multiple languages are supported, and the workout will begin when the user clicks the play button. Once started, The user can highly customize how their preferred coach is set up, the video size, color background, etc.</p><p><img src="/images/stm403,figure2.png" title="Figure 2: The Diagram of Our Draft Mobile Fitness Application Designed for Sign Up"></p><p>Our IT member simply marked, added, and edited each interface and interaction. This is not a programmed functionality application that involves any programming language. We use Adobe XD, a prototyping tool, to make the user interface, transitions, and interactive buttons with less cost and time.<br>Since the Bison is the proprietary logo of Gallaudet University, we decided to call the app: Bison Fitness once we get the permission to use it. Figure 2 shows a number of ways users can sign up or log in to confirm so we can validate that the user who is attempting to log in is a Gallaudet University student. The user will then be prompted to enter information such as the body emphasis region of emphasis for the exercise, the primary goal of doing this, and which body part they want to gain and grow muscle on.</p><p>Figure 3 shows more interactive options for users, including videos and replays to build muscle or whatever they need. Users can choose a workout method. After the user selects a workout plan, it will show what they need to prepare, such as a yoga mat, dumbbells, a healthy diet, water, or other equipment, before starting. The exercise video and timer will follow. The viewer can pause, skip, and stop the video and request more counts. Rest, then return to the main website to start the next workout level.</p><p>We provide athletes teams with the app in Figure 3, the fifth diagram of the workout lesson can be found by the coach with his set. This app is useful for all Gallaudet athletes teams to track their workout plan/ lesson with the video guideline instructions, including ASL, pictures, etc. These would help them to avoid any injury. When the team was assigned to do a workout lesson created by the coach, they could start the workout by following the instruction. They can’t pretend to finish it by hand. Its app would display the workout video with the time based on the settings made by coaches. The team has to finish their workout lesson before the time out, so it will go on the completed list. Coaches can see whether they are finished or not.</p><p><img src="/images/stm403,figure3.1.png"></p><p><img src="/images/stm403,figure3.2.png" title="Figure 3: The Diagram of Our Draft Workout Choice at Homepage"></p><h3 id="Mobile-Application-Evaluation"><a href="#Mobile-Application-Evaluation" class="headerlink" title="Mobile Application Evaluation"></a>Mobile Application Evaluation</h3><p>The evaluation was set in a few parts (Figure 4). The introduction to the evaluation and training mobile application was set at the beginning. Then came the actual main evaluation session and, finally, the Q&amp;A session. Each participant was notified that there is no compensation for their time during the survey because we do not get support from organizations and departments. All of the things are volunteered. The feedback and survey remained confidential and was only to be shared among group members of this project.</p><p><img src="/images/stm403,figure4.PNG" title="Figure 4: A Flowchart Showing The Process of the Evaluation"></p><ol><li>First, we meet the participants and explain the Informed Consent and Feedback Release form in ASL or spoken English. A general explanation of the project’s purpose was to be shared as follows: “In our STM Project, we are conducting studies of different people’s user experience and feedback on Gallaudet fitness center to promote accessibility for the on-campus/off-campus students and Gallaudet community.”</li><li>The participants will be seated in a room and can use electronic devices (smartphone/laptop) provided by our STM project group or bring their own devices to use.</li><li>The group member will explain the process of how to perform the application on a computer and mobile phone during the evaluation by showing the participants how to use and interact with them. Additionally, participants will expect to complete a Google Form survey and a short interview Q&amp;A within a few minutes.</li><li>The expected time to be completed will be around 10 minutes to finish all the steps in this evaluation. If the participant is interested in the apps, we can provide them with the IPA IOS file to be installed on their phone. They can continue using our app for a week and return later for more feedback.</li></ol><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>Since this research involved the human subject and collecting data from their feedback, and despite the fact that we still needed to be granted approval from the IRB, we had both the finished prerequisite survey and the process for conducting the evaluation ready to be implemented. Although we did not get the IRB approval, excluding the data we have yet to collect from participants, we did come across a lot of research from various sources. We discovered that the lack of on-campus exercise options at Gallaudet University, such as the limited schedule time (Gallaudet Athletics Weight Room 2022) and low, convenient commute between the fitness center and dorm, has resulted in a decrease in the number of people working out and has created an environment that discourages people from working out. Another result we discussed in our group was a member as a soccer player at Gallaudet University with other athletic teams: Charles experienced that the given workout plan and lessons provided by the coach were written on the whiteboard, which was ineffective for working out individually and with other athletic members. We also discovered that having a mobile device with a fitness app can significantly motivate and will gradually improve students’ mental and physical health (Wharton et al,. 2008), but the majority of apps don’t use exercises according to ACSM guidelines, which is likely to have an impact on the user’s health and results in inefficient workouts of low quality (Guo et al., 2022).</p><p>We have created the preliminary design via Adobe XD for the mobile application. Figure 5 shows the login page to verify the user’s identity and the home page, along with all of its essential features. Please note that this is not the final stage; further development with fresh concepts and features is worthwhile. The majority of the problems we currently have been solved by the mobile app.. It also complies with the prevalent use of fitness apps in the deaf community and increases the effectiveness of exercise with ACSM guidelines.</p><p><img src="/images/stm403,figure5.png" title="Figure 5: The Current Progress of Bison Fitness’ Login/Home Page Interface"></p><p>Our application prototype is finished and available for full interaction. This link <a href="https://xd.adobe.com/view/2f363568-09f5-4109-b326-a4765c4e20fc-7a65/">(Click here)</a> will take you to a demonstration of the application. When the Adobe XD prototype is opened, the viewer can use a mouse or trackpad to interact with the application’s buttons, much like on a real smartphone. Suppose the prototype receives clicks on areas that are not intended for clicking. In that case, blue boxes will appear on those that provide instructions to the viewer if they need clarification about how to interact with the prototype.</p><h3 id="Discussion-Conclusion"><a href="#Discussion-Conclusion" class="headerlink" title="Discussion/Conclusion"></a>Discussion/Conclusion</h3><p>In conclusion, the use of a fitness mobile application with ACSM guidelines and regular exercise with detailed instructions and video tutorials, combined with the support of a sign language interpreter, can bring numerous benefits to individuals who are deaf or hard of hearing. Exercise can improve physical health by increasing cardiovascular endurance, strengthening muscles, and reducing the risk of chronic diseases. Additionally, regular exercise has been shown to improve mental health by reducing stress and anxiety, improving mood, and increasing self-esteem. A mobile fitness application, when used with the support of a sign language interpreter, can provide deaf or hard-of-hearing individuals with access to important health and fitness information, allowing them to track and monitor their physical activity, including different workout plans that can be set by an athletics coach or by the individual, and receive personalized workout recommendations. Overall, using a fitness mobile application and regular exercise, with the support of a sign language interpreter, can improve physical and mental health for deaf or hard-of-hearing individuals, making it an important aspect of their overall well-being.</p><h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><p>A further step in the evaluation and analysis of the data is needed, even if we get approval from the IRB approval which only covered a small portion of the data we had collected. So a survey with more varied questions with more participants is necessary to collect more details about fitness.</p><p>We encourage participants and other volunteer individuals who are fluent in ASL or another sign language to create instructional workout videos that we can later embed their video in our mobile app to make the app more diverse and support more sign language in the deaf community. We will also develop and implement the goal we set for ourselves, which includes incorporating the ACSM guideline, information on physical activity and nutrition, and the possibility of adding new ideas in the future.</p><p>We strongly encourage anyone interested and the Gallaudet student life skill department in our project during the spring semester of 2023, when it will be carried out in conjunction with the IT Senior Capstone course. Our members who majored in IT will develop the programmed mobile application with Swift, which will require a significant amount of fitness data and references.</p><h3 id="Roles"><a href="#Roles" class="headerlink" title="Roles"></a>Roles</h3><p>Abraham Cervantes: I will support and assist any group members who need it. Prepare the tool that will be used to collect data on the arriving participants, such as using Google Sheet to produce graphs or tools to build tables. On the mobile application, create a data diagram, chart, and graphic concept that contains statistical information for the user. It visually balances time and speed in terms of kilometers traveled. My experience at Gallaudet University improved, and I was able to provide for Bison Fitness by contributing to our project group.</p><p>Charles Harris: My duties included researching American College of Sports Medicine (ACSM) guidelines for physical activities and mental well-being and making some examples of ACSM guidelines in the presentation. Gathered more information from good sources with peer-reviewed on how to exercise app beneficial to physical health and mental health. Created the questionnaire survey about 24/7 fitness, our mobile app, then made visionary data (Unfortunately, we have yet to hear from IRB for approval that we could not get data). I also made a presentation with design and helped some with the final paper.</p><p>Zehui Liu: It is my responsibility to come up with the idea for the mobile app as well as any new concepts, draw them all into some flowcharts to make them more visualized, and then hand them over to Obaidullah so that he can design the application. In addition to this, it is my responsibility to work on and revise the draft of the project proposal as well as write the final paper.</p><p>Obaidullah Bakali: In this project, as I am majoring in Information Technology, I am responsible for the technological aspect. I also am responsible for scheduling in-person or Zoom meetings to ensure that we all work together on the same page. I researched and reviewed several mobile designs for this project to improve the UI design. Since I have the Adobe XD prototype on my laptop, I am mainly focused on designing the mobile app and getting input from the three members of my group. In the project pitch, I helped organize concepts to include. In this proposal, I typed the issues section while we were discussing it in the group meeting and added some to the methods and results section.</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><table>    <tr>        <td>Chin, J. P., Diehl, V. A., & Norman, L. K. (1988). Development of an Instrument measuring user satisfaction of the human-computer interface. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '88), https://doi.org/10.1145/57167.57203        </td>    </tr>     <tr>        <td>Cugusi, L., Di Blasio, A., & Bergamin, M. (2021). The Social Media Gym-class: Another lesson learnt from covid-19 lockdown. Sport Sciences for Health, 17(2), 487–488. https://doi.org/10.1007/s11332-021-00747-6.        </td>    </tr>     <tr>        <td>Dalle Grave R. (2020). Nutrition and Fitness: Mental Health. Nutrients, 12(6), 1804. https://doi.org/10.3390/nu12061804.        </td>    </tr>    <tr>        <td>Ehrlén, V. (2020, December 11). Full article: Tracking oneself for others: Communal and self ... Taylor& Francis Online. Retrieved December 14, 2022, from https://www.tandfonline.com/doi/full/10.1080/02614367.2020.1869289        </td>    </tr>        <tr>        <td>Evenepoel, C., Clevers, E., Deroover, L., Van Loo, W., Matthys, C., & Verbeke, K. (2020). Accuracy of Nutrient Calculations Using the Consumer-Focused Online App MyFitnessPal: Validation Study. Journal of Medical Internet Research, 22(10), e18237. https://doi.org/10.2196/18237        </td>    </tr>        <tr>        <td>Gowin, M., Cheney, M., Gwin, S., & Wann, T. F. (2015). Health and Fitness App Use in College Students: A Qualitative Study. American Journal of Health Education, 46(4), 223-230. https://proxyga.wrlc.org/login?url=https://www.proquest.com/scholarly-journals/health-fitness-app-use-college-students/docview/1697733807/se-2.        </td>    </tr>            <tr>        <td>Gowin, M., Cheney, M., Gwin, S., & Wann, T. F. (2014, December 30). Health and fitness app use in college students: A qualitative study. Taylor&Francis Online. Retrieved December 14, 2022, from https://www.tandfonline.com/doi/full/10.1080/19325037.2015.1044140        </td>    </tr>            <tr>        <td>Guo1, Y., Bian2, J., Leavitt3, T., Vincent3, H. K., Zalm3, L. V., Teurlings3, T. L., Smith2, M. D., Modave2*, F., Institute of Natural Resources and Environmental Audits, & Guo, C. A. Y. (2017, March 7). Assessing the quality of mobile exercise apps based on the American College of Sports Medicine Guidelines: A reliable and valid scoring instrument. Journal of Medical Internet Research. Retrieved from https://www.jmir.org/2017/3/e67/.        </td>    </tr>            <tr>        <td>Hoon, L., Vasa, R., Schneider, J.-G., & Grundy, J. (1970, January 1). An analysis of the mobile app review landscape: Trends and implications. Semantic Scholar. Retrieved from https://www.semanticscholar.org/paper/An-Analysis-of-the-Mobile-App-Review-Landscape%3A-and-Hoon-Vasa/3477d88a8e5f3af97efad587951fc8ca56b3efbb.        </td>    </tr>            <tr>        <td>Irwin, J.D. (2004) “Prevalence of University Students' Sufficient Physical Activity: A Systematic Review,” Perceptual and Motor Skills, 98(3), pp. 927–943. Available at: https://doi.org/10.2466/pms.98.3.927-943.        </td>    </tr>            <tr>        <td>Kinney, D. (2017). College Students Use and Perceptions of Wearable Fitness Trackers and Mobile Health Apps [Doctoral dissertation, University of Cincinnati]. OhioLINK Electronic Theses and Dissertations Center. http://rave.ohiolink.edu/etdc/view?acc_num=ucin1504798862580571.        </td>    </tr>                <tr>        <td>Muscaritoli M. (2021). The Impact of Nutrients on Mental Health and Well-Being: Insights From the Literature. Frontiers in nutrition, 8, 656290. https://doi.org/10.3389/fnut.2021.65629        </td>    </tr>                <tr>        <td>PrestoSports, MedStarSportMedcine, ECFC, UnitedEast, NCAA. (n.d.). Gallaudet Atheletics Weightroom. Gallaudet Athletics. Retrieved December 13, 2022, from https://www.gallaudetathletics.com/facilities/root/weight/index.html.        </td>    </tr>                <tr>        <td>Romero*, R. L., Kates*, F., Hart*, M., Ojeda, A., Meirom, I., & Hardy\*, S. (2019, October 30). Quality of deaf and hard-of-hearing mobile apps: Evaluation using the mobile app rating scale (MARS) with additional criteria from a content expert. JMIR mHealth and uHealth. Retrieved December 14, 2022, from https://mhealth.jmir.org/2019/10/e14198/.        </td>    </tr>                <tr>        <td>Thompson, P. D., Arena, R., Riebe, D., & Pescatello, L. S. (2013). ACSM’s New Preparticipation Health Screening Recommendations from ACSM’s Guidelines for Exercise Testing and Prescription, Ninth Edition. Current Sports Medicine Reports, 12(4), 215–217. https://doi.org/10.1249/jsr.0b013e31829a68cf        </td>    </tr>                    <tr>        <td>Tong HL, Maher C, Parker K, Pham TD, Neves AL, Riordan B, et al. (2022) The use of mobile apps and fitness trackers to promote healthy behaviors during COVID-19: A cross-sectional survey. PLOS Digit Health 1(8): e0000087. https://doi.org/10.1371/journal.pdig.0000087.        </td>    </tr>                <tr>        <td>Turner, A., & Comfort, P. (Eds.). (2022). Advanced Strength and Conditioning: An Evidence-based Approach (2nd ed.). Routledge. https://doi.org/10.4324/9781003044734.        </td>    </tr>                <tr>        <td>Wharton, C. M., Adams, T., & Hampl, J. S. (2008). Weight loss practices and body weight perceptions among US college students. Journal of American College Health, 56(5), 579–584. https://doi.org/10.3200/jach.56.5.579-584.        </td>    </tr>                <tr>        <td>Yang, C.-H., Maher, J. P., & Conroy, D. E. (2015). Implementation of behavior change techniques in mobile applications for physical activity. American Journal of Preventive Medicine, 48(4), 452–455. https://doi.org/10.1016/j.amepre.2014.10.010.        </td>    </tr></table>]]></content>
      
      
      <categories>
          
          <category> STM403 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Articles </tag>
            
            <tag> Mobile Application </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Annotated Bibliography</title>
      <link href="/2022/10/31/STM403/Annotated_Bibliography/"/>
      <url>/2022/10/31/STM403/Annotated_Bibliography/</url>
      
        <content type="html"><![CDATA[<h3 id="Research-Topics-and-Questions"><a href="#Research-Topics-and-Questions" class="headerlink" title="Research Topics and Questions"></a>Research Topics and Questions</h3><p>As a person who did not have any previous experience with mobile development, I am curious as to how I can combine the information that I have gained thus far with the knowledge that I will gain in the future in fall semester in order to incorporate the works into our project. At the same time, there needs to be a certain contribution made to speed up the project. Therefore, a predetermined portion of the available resources must be used as a point of reference for me to be able to continue with my progress, with the high standard of quality such as ease of use and accessibility on mobile application with as few roadblocks as is humanly possible.</p><h3 id="Two-Journal-articles"><a href="#Two-Journal-articles" class="headerlink" title="Two Journal articles:"></a>Two Journal articles:</h3><p><b>SHNEIDERMAN, B. E. N. (2007). We can design better user interfaces: A review of Human-Computer Interaction Styles. Taylor &amp; Francis Online, 31(5). <a href="https://doi.org/10.1080/00140138808966713">https://doi.org/10.1080/00140138808966713</a></b></p><p>The author offers the following three pillars to help the user interface design process. The author encourages us to focus more attention and time on the HCI to direct manipulation in which the objects and actions are visible with three pillars: Documents serving as guidelines, a User Interface Management System, and usability laboratories for repeated testing are all essential. This article review can be used as a reference to provide us with some ideas about the kind of interface that we need to develop, so keep that in mind as we go through it. It is really helpful for me in terms of the development of mobile apps, which means that we may incorporate them into our project.</p><p><b>Chin, J. P., Diehl, V. A., &amp; Norman, L. K. (1988). Development of an instrument measuring user satisfaction of the human-computer interface. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI ‘88, CHI(Proceedings). <a href="https://doi.org/10.1145/57167.57203">https://doi.org/10.1145/57167.57203</a></b></p><p>Experiments of both a quantitative and qualitative nature are being conducted in this article, with participants coming in to evaluate various aspects of the interface. After completing the series of tasks, a questionnaire will be presented for the interface satisfying of using the application. The researcher will provide the data graphs to demonstrate that the interface is easy to use or not. When we are still in the preliminary stages of developing our mobile app, using this methodology to collect information on the ease of use should prove to be highly beneficial to our project.</p><h3 id="One-book-chapter"><a href="#One-book-chapter" class="headerlink" title="One book chapter"></a>One book chapter</h3><p><b>Roth, R. M., Dennis, A., Wixom, B. H., Wixom, B. H., &amp; Wixom, B. H. (2013). User Interface Design Process, Interface Structure Design. In Systems analysis and design (pp. 325–327). essay, John Wiley. </b></p><p>ISD (Interface Structure Diagram) are used to demonstrate how all of the screens, forms, and reports that are used by the system are related to one another as well as how the user navigates from one to the next. The majority of systems consist of multiple ISDs, one for each significant component of the system. Lines connecting each interface to its neighboring interfaces demonstrate to users how they can go from one screen to the next in the hierarchy of interfaces. The visualized diagram may be of use to the developer in the process of developing the project idea. It should be a great way for me to use while working on this group project. It provided an explanation of how to use the design and how to ensure that the correct messages and design were delivered to OB. This can be allowed OB to understand what my thoughts were and also allow him/other group members to use my design diagram to adjust and fix by their ideas.</p><h3 id="A-collection-of-some-review-articles-related-to-HCI"><a href="#A-collection-of-some-review-articles-related-to-HCI" class="headerlink" title="A collection of some review articles related to HCI"></a>A collection of some review articles related to HCI</h3><p><b>Punchoojit, L., &amp; Hongwarittorrn, N. (2017, November 9). Usability studies on Mobile User Interface Design Patterns: A Systematic Literature Review. Advances in Human-Computer Interaction. Retrieved October 30, 2022, from <a href="https://www.hindawi.com/journals/ahci/2017/6787504/">https://www.hindawi.com/journals/ahci/2017/6787504/</a> </b></p><p>The author is of the opinion that there are no standards for creating a consistent user interface for mobile applications; consequently, he conducted a systematic literature review of the design of mobile user interfaces before gathering additional details and getting ready to conduct their research. It offered more than a hundred different reviews of articles and let me read any of them along with a brief summary of the author’s thoughts on the work. To look for what materials I need to locate from this article review is something that has shown to be quite helpful for me.</p><h3 id="Resources-taken-from-the-notes"><a href="#Resources-taken-from-the-notes" class="headerlink" title="Resources taken from the notes"></a>Resources taken from the notes</h3><p><b>Rae, M. (2020, July 27). Mobile design with Adobe XD. Adobe.com. Retrieved October 27, 2022, from <a href="https://www.adobe.com/products/xd/learn/design/layout/adobe-xd-for-mobile-design.html">https://www.adobe.com/products/xd/learn/design/layout/adobe-xd-for-mobile-design.html</a> </b></p><p>For first-time developers, instructions and guidelines for creating the user interface (UI) by utilizing Adobe Xd, along with transition graphics and text-based explanations, are provided. It can learn how to make a UI with just a few basic functions, the interaction buttons can be utilized, etc., which can speed up the learning of Adobe Xd. It is similar to a brief tutorial, but it has been formatted into an “article-like” tutorial. Therefore, with this kind of tutorial, I may become familiar with the components that comprise immersive experiences. Which is a terrific point for someone who is just starting off like me.</p><p><b>Smith, A. (2021, June 30). Adobe XD vs sketch — a comparison about the good the bad the more fitting. uxplanet.org. Retrieved October 30, 2022, from <a href="https://uxplanet.org/adobe-xd-vs-sketch-a-comparison-about-the-good-the-bad-the-more-fitting-451fb155e2">https://uxplanet.org/adobe-xd-vs-sketch-a-comparison-about-the-good-the-bad-the-more-fitting-451fb155e2</a> </b></p><p>Examine two programs, Adobe Xd and Sketch, and provide input on the benefits and drawbacks of employing each in the capacity of a moderately experienced developer. Along with this, include some comments for those who are just starting out. The author presented a number of the one-of-a-kind functions that can only be employed by each software itself, and he or she indicated whether or not they are worth learning and putting into practice. It may be a fantastic start for me to pick which program to use to make the UI design that is best for me. I will take both the benefits and negatives to compare, and I will also check to see whether OB is in agreement with me regarding these functions.</p><p><b>jtenenbg. (2018). Shneiderman’s “Eight Golden Rules of Interface Design”. Washington Education. Retrieved October 30, 2022, from <a href="https://www.cs.utexas.edu/users/almstrum/cs370/elvisino/rules.html">https://www.cs.utexas.edu/users/almstrum/cs370/elvisino/rules.html</a> </b></p><p>The more concise and easy-to-understand rules that enable developers to rapidly comprehend the necessary features that need to be added or adjusted without having to read a great deal of text and information, and it had been extensively used all over the world. In order for the other members of the group to be able to help us with some basic evaluation and provide some feedback on our mobile app design, we ask them to first understand what interface design is and then show them this paper as an excellent way of how to do so without requiring them to spend a significant amount of time.</p><h3 id="Professional-websites-and-possible-resources"><a href="#Professional-websites-and-possible-resources" class="headerlink" title="Professional websites and possible resources"></a>Professional websites and possible resources</h3><p><b>Yoo, H., &amp; Lee, Y. (2017). An Automatic Mobile App Testing Method with User Event Scenario. CSDL | IEEE Computer Society. Retrieved October 31, 2022, from <a href="https://www.computer.org/csdl/proceedings-article/mdm/2017/07962487/12OmNB1NVNv">https://www.computer.org/csdl/proceedings-article/mdm/2017/07962487/12OmNB1NVNv</a> </b></p><p>According to the author, the tool that was utilized by the team and referred to as “Monkey” is a fundamental instrument that can test and forecast the user behaviors, reactions, and actions of interacting with the Mobile app in a variety of scenarios, but Monkey does not have enough of the ability for testing complex situation. So the author developed an innovative strategy that is capable of being implemented with greater efficiency and a more rapid pace of testing. Despite the fact that it is test on Android devices, the concept may be used on our mobile application development to gauge the response of the participants.</p><p><b>Ruderman, R. (2022, February 17). Teach Access Tutorial. Access Computing. Retrieved October 31, 2022, from <a href="https://teachaccess.github.io/tutorial/">https://teachaccess.github.io/tutorial/</a> </b></p><p>It is a professional website that guides developers and users to interact with the accessibility on web and mobile applications. This website has clear instructions for using which kind of software is the best suit for uses, and it also maintains the design principles. This can be used as a guideline on the accessibility on video, the text for users who are deaf or blind. Nevertheless, I believe that it will be of some assistance to me, and I intend to make use of it as a reference while I collaborate with another member of my group, OB, to design a mobile application that is more accessible to disabilities people.</p><p><b>Savory, A. (2014, October 1). Develop hybrid mobile applications with Apache Cordova and PhoneGap Enterprise with Andrew Savory. Association for Computing Machinery. Retrieved October 31, 2022, from <a href="https://learning.acm.org/techtalks/mobile">https://learning.acm.org/techtalks/mobile</a> </b></p><p>The Association for Computer Machinery (ACM) is the largest computing society in the world, and its extensive learning center and special content can be of great assistance to students like myself. This article, which provides brief summary of creating a hybrid application by making use of Apache Cordova, is an excellent illustration of how to get started with mobile app development. I won’t get into these much more in-depth topics, but I do plan to spend about half an hour watching the presentation and figuring out what the point of creating the hybrid mobile app application is and how I can apply it to our work.</p>]]></content>
      
      
      <categories>
          
          <category> STM403 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Articles </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ITS395 Quiz2</title>
      <link href="/2022/10/16/Accessibility/Quiz2/"/>
      <url>/2022/10/16/Accessibility/Quiz2/</url>
      
        <content type="html"><![CDATA[<ol><li><p>According to the “Web accessibility: Filtering redundant and irrelevant information improves website usability for blind users” article, what is the impact of information filtering for blind and low vision users’ cognitive load?</p><ul><li>substantially increases cognitive load</li><li><mark class="hl-label blue">substantially decreases cognitive load</mark> </li><li>does not increase or decrease cognitive load</li><li>has less than substantial increase in cognitive load</li></ul></li><li><p>According to the “Accessible Images For When They Matter Most” article, what is the percentage of visual learners?</p><ul><li>25%</li><li>50%</li><li><mark class="hl-label blue">65%</mark> </li><li>90%</li></ul></li><li><p>According to the “Article - Paint the Picture, Not the Frame: How Browsers Provide Everything Users Need”, users who are told that they were rejected by the system rather than another human are more likely to be</p><blockquote><p>It could be angry or morose, not sure if they are correct or not</p></blockquote><ul><li><mark class="hl-label blue">angry</mark> </li><li>happy</li><li>sad</li><li><mark class="hl-label blue">morose</mark> </li></ul></li><li><p>In the “Conversational Semantics” article, which HTML elements are considered “phrasing” elements?</p><ul><li>head</li><li>body</li><li>footer</li><li><mark class="hl-label blue">em</mark> </li></ul></li><li><p>In the “Conversational Semantics” article, what is the main benefit of the <lang> element for screen-readers?</p><ul><li><mark class="hl-label blue">inflecting the word with the correct accent</mark> </li><li>translating the word to the correct language</li><li>No benefit</li><li>translating and inflecting the word</li></ul></li><li><p>In the book “Web Accessibility : Web Standards and Regulatory Compliance”, what accessible technology feature can help users with multiple sclerosis?</p><ul><li><mark class="hl-label blue">sclerotic feature</mark> </li><li>mouth stick</li><li>keyboard</li><li>voice</li></ul></li><li><p>In the book “Web Accessibility : Web Standards and Regulatory Compliance”, the purpose of alt text is fully accomplished when:</p><ul><li>the page is more useful with images</li><li>the page is more useful with alt text</li><li><mark class="hl-label blue">the page is equally useful with images or alt text</mark> </li><li>the page is accessible to everyone with images</li></ul></li><li><p>What is the best alt text description for a search icon next to the search bar at the front page of the Acme Company Website?</p><ul><li>“This image is a line art drawing of a dark green magnifying glass. If you click on it, it will take you to the Search page for this Acme Company website.”</li><li><mark class="hl-label blue">Search""</mark> </li><li>“Image of a Search Icon. If you click on it, it will take you to the search page for this Acme Company website.”</li><li>“Image of a Search Icon.”</li></ul></li><li><p>What is the definition of usability according to ISO 9421-11 as outlined in the Web Accessibility textbook?<br>-“extent to which a product is usable”</p><ul><li><mark class="hl-label blue">“extent to which a product can be used by specified users to achieve specified goals effectively, efficiently and with satisfaction in a specified context of use”</mark> </li><li>“extent to which a product is accessible”</li><li>“extent to which a product meets user needs”</li></ul></li><li><p>Usability problems impact:</p><p>-users with disabilities only<br>-users with no disabilities</p><ul><li><mark class="hl-label blue">users with and without disabilities</mark> </li><li>users and developers</li></ul></li><li><p>Per the Web Accessibility textbook, accessibility problems impact:</p><ul><li><mark class="hl-label blue">users with disabilities only</mark> </li><li>users with no disabilities</li><li>users and developers</li><li>users, developers and customers</li></ul></li><li><p>Per the Web Accessibility textbook, are text-only pages an acceptable solution?</p><ul><li>True</li><li><mark class="hl-label blue">False</mark> </li></ul></li><li><p>Per the Web Accessibility textbook, a statute is a:</p><ul><li><mark class="hl-label blue">legislative law</mark> </li><li>case law</li><li>administrative regulation</li><li>guideline</li></ul></li><li><p>Per the Web Accessibility textbook, what are the US Web Accessibility Standards known as?</p><ul><li>Section 225</li><li>Section 504</li><li><mark class="hl-label blue">Section 508</mark> </li><li>Section 911</li></ul></li><li><p>What is the full name for PDF?</p><ul><li>Printable Document Format</li><li><mark class="hl-label blue">Portable Document Format</mark> </li><li>Picture Document Format</li><li>Pretty Document Format</li></ul></li><li><p>What command do you run to convert scanned images of text to actual text?</p><ul><li>“PDF”</li><li>“tags”</li><li><mark class="hl-label blue">OCR""</mark> </li><li>“HTML”</li></ul></li><li><p>In the article: “Accessible Images For When They Matter Most”, how do you mark decorative images using alt img?</p><ul><li>alt img=”alt”</li><li><mark class="hl-label blue">alt img=</mark> </li><li>alt img=”SSN”</li><li>alt /</li></ul></li><li><p>In the article: “Accessible Images For When They Matter Most”, how many people worldwide are color-blind?</p><ul><li>100 million</li><li>200 million</li><li><mark class="hl-label blue">300 million</mark> </li><li>400 million</li></ul></li><li><p>People with dyslexia benefit from:</p><ul><li>alt text</li><li>alt img</li><li>kerning</li><li><mark class="hl-label blue">typography</mark> </li></ul></li><li><p>Creating accessible images requires more than just adding alt text.</p><ul><li><mark class="hl-label blue">True</mark> </li><li>False</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Accessibility Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Accessibility </tag>
            
            <tag> Quizzes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Structures</title>
      <link href="/2022/10/11/Algorithm/Data_Structures/"/>
      <url>/2022/10/11/Algorithm/Data_Structures/</url>
      
        <content type="html"><![CDATA[<h3 id="Data-Structure"><a href="#Data-Structure" class="headerlink" title="Data Structure"></a>Data Structure</h3><p>Definition: A data structure is a set of objects, relationships between objects, and a set of operations that can be applied to the set.<br>Examples of objects:</p><div class="note modern"><p>numbers(int, double,…), strings, instances of classes(Cars, Customers,…), etc.</p></div><p>Examples of relationships:</p><div class="note modern"><p>next, previous, left, right, up, down, etc.</p></div><p>Examples of operations:</p><div class="note modern"><p>insert, search, delete, etc.</p></div><p>Examples of data structures:</p><div class="note modern"><p>arrays, stacks, queues, heaps, trees, hash tables, …</p></div><h3 id="Arrays-and-Lists"><a href="#Arrays-and-Lists" class="headerlink" title="Arrays and Lists"></a>Arrays and Lists</h3><p>Array is a very basic data structure consisting of a set of similar objects, accessed by index.</p><p>Advantages of arrays:</p><ul><li>effectively stored inside the computer, no overhead per object(no space is wasted).</li><li>fast access(by the index) to all of its objects at O(1) time.</li></ul><p>Disadvantages of arrays:</p><ul><li>not completely dynamic (fixed size arrays)</li><li>Insertion and deletion of an element in the array requires to shift O(n) elements on</li><li>average, where n is size of the array.</li></ul><h4 id="Arrays-in-Python"><a href="#Arrays-in-Python" class="headerlink" title="Arrays in Python"></a>Arrays in Python</h4><div class="tabs" id="test4"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test4-1">Program</button></li><li class="tab"><button type="button" data-href="#test4-2">Output</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test4-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">myArray = np.zeros(<span class="number">4</span>, dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line"><span class="comment">#creates an array of type int32, size 4, and initialize it to 0</span></span><br><span class="line">myArray[<span class="number">0</span>] = <span class="number">6</span></span><br><span class="line">myArray[<span class="number">1</span>] = <span class="number">7</span></span><br><span class="line">myArray[<span class="number">2</span>] = <span class="number">8</span></span><br><span class="line">myArray[<span class="number">3</span>] = <span class="number">9</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;myArray size is: &quot;</span>, myArray.size )</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, myArray.size, <span class="number">1</span>):</span><br><span class="line">    <span class="built_in">print</span>(myArray[i])</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-2"><div class="note modern"><p>myArray size is: 4<br>6<br>7<br>8<br>9</p></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h4 id="Lists-in-Python"><a href="#Lists-in-Python" class="headerlink" title="Lists in Python"></a>Lists in Python</h4><p>In Python,  lists are very similar to arrays but they are implemented differently. </p><p>Lists are dynamic, the size is not fixed and they can have more than one data type in the same list. Lists require overhead (in space and time).</p><div class="tabs" id="test4"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test4-1">Program</button></li><li class="tab"><button type="button" data-href="#test4-2">Output</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test4-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">List1 = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">0</span>, <span class="built_in">len</span>(List1)):</span><br><span class="line">    <span class="built_in">print</span>(List1[i])</span><br><span class="line"></span><br><span class="line">List1.append(<span class="number">20</span>)</span><br><span class="line">List1.insert(<span class="number">2</span>, <span class="number">40</span>)</span><br><span class="line">List1.remove(<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(List1)</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-2"><div class="note modern"><p>2<br>4<br>6<br>8<br>10<br>[2, 4, 40, 8, 10, 20]</p></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h4 id="Two-Dimensional-Arrays-in-Python"><a href="#Two-Dimensional-Arrays-in-Python" class="headerlink" title="Two-Dimensional Arrays in Python"></a>Two-Dimensional Arrays in Python</h4><p>In an n-dimensional array, the index of each element is an n-tuple of positive integer numbers. </p><p>The most commonly used multidimensional array is the two-dimensional array where the index of each element is an ordered pain of positive integers. </p><p>Two-dimensional arrays are also knowns as  tables or matrices. </p><div class="tabs" id="test4"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test4-1">Program</button></li><li class="tab"><button type="button" data-href="#test4-2">Output</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test4-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">matrix = np.zeros((<span class="number">2</span>, <span class="number">3</span>), dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line"><span class="comment">#creates a matrix  of dimension 2, type int, and</span></span><br><span class="line"><span class="comment">#initialize it to 0&#x27;s</span></span><br><span class="line">matrix[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">3</span></span><br><span class="line">matrix[<span class="number">0</span>][<span class="number">1</span>] = <span class="number">4</span></span><br><span class="line">matrix[<span class="number">0</span>][<span class="number">2</span>] = <span class="number">5</span></span><br><span class="line">matrix[<span class="number">1</span>][<span class="number">0</span>] = <span class="number">6</span></span><br><span class="line">matrix[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">7</span></span><br><span class="line">matrix[<span class="number">1</span>][<span class="number">2</span>] = <span class="number">8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="built_in">print</span>(matrix[i][j], end =<span class="string">&quot; &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;matrix dimension is: &quot;</span>, matrix.ndim)</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-2"><div class="note modern"><p>3 4 5<br>6 7 8<br>matrix dimension is:  2</p></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h3 id="Queues"><a href="#Queues" class="headerlink" title="Queues"></a>Queues</h3><p>A queue is a data structure that supports the operations:</p><ol><li>enqueue(x)// insert x into stack</li><li>dequeue(x) // delete x from stack</li></ol><p><img src="/images/Algorithm/Queues1.png"></p><div class="note modern"><p>A Queue helps to manage the data in the  First In First Out (FIFO) method.<br>In Queue, random access is not possible</p></div><p>A queue can be implemented  as a simple array or as a class.<br>To implement the operations of a queue, one can maintain two global indices: head, and tail.  </p><p>The basic operations can be coded as follows:</p><div class="tabs" id="test4"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test4-1">Enqueue</button></li><li class="tab"><button type="button" data-href="#test4-2">Dequeue</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test4-1"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Procedure enqueue(Q, a)</span><br><span class="line">begin</span><br><span class="line">   tail = tail +1; </span><br><span class="line">   Q[tail] = a;</span><br><span class="line">end</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-2"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Procedure dequeue(Q)</span><br><span class="line">begin</span><br><span class="line">  head = head + 1; </span><br><span class="line">end</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h4 id="Priority-Queues"><a href="#Priority-Queues" class="headerlink" title="Priority Queues"></a>Priority Queues</h4><p><img src="/images/Algorithm/Queues2.png"></p><p><b>Example:</b> Suppose we have the following 6 jobs with their priorities:</p><div class="note modern"><p>A-2, B-3, C-2, D-1, E-2, F-2 </p></div><p>Suppose these jobs are scheduled using a regular queue by performing the following operations:</p><div class="note modern"><p>enqueue(A), enqueue(B), enqueue(E), enqueue(C), enqueue(D), enqueue(F) </p></div><p>Now suppose the jobs are processes by calling the dequeue operation.</p><p>These jobs will be processes look like this: </p><div class="note modern"><p>F - D - C - E - B - A</p></div><h3 id="Singly-Linked-Lists"><a href="#Singly-Linked-Lists" class="headerlink" title="Singly Linked Lists"></a>Singly Linked Lists</h3><p>A <b>linked list</b> is a linear data structure, in which the elements are not stored at contiguous memory locations. </p><p>Every node of a singly-linked list contains  a value and a link to the next element.</p><p><img src="/images/Algorithm/Linked_Lists.png"></p><p><b>Example</b>: Supposed we want to store the following numbers in a singly-linked list:</p><div class="note modern"><p>6, 8, 9, 12, 56, 78</p></div><p>Then we can store the information as (value, link) as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#assuming that 0 represents a null link</span></span><br><span class="line">V[<span class="number">1</span>] = <span class="number">6</span>, L[<span class="number">1</span>] = <span class="number">2</span></span><br><span class="line">V[<span class="number">2</span>] = <span class="number">8</span>, L[<span class="number">2</span>] = <span class="number">3</span></span><br><span class="line">V[<span class="number">3</span>] = <span class="number">9</span>, L[<span class="number">3</span>] = <span class="number">4</span></span><br><span class="line">V[<span class="number">4</span>] = <span class="number">12</span>, L[<span class="number">4</span>] = <span class="number">5</span></span><br><span class="line">V[<span class="number">5</span>] = <span class="number">56</span>, L[<span class="number">5</span>] = <span class="number">6</span></span><br><span class="line">V[<span class="number">6</span>] = <span class="number">78</span>, L[<span class="number">6</span>] = <span class="number">0</span> </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Articles </tag>
            
            <tag> Code </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Article Review</title>
      <link href="/2022/10/10/STM403/ArticleReview/"/>
      <url>/2022/10/10/STM403/ArticleReview/</url>
      
        <content type="html"><![CDATA[<p>The article focused on the captions and sign language video to see if the video with both captions and a sign language interpreter is better than the video with just the interpreter. The author is trying to conduct some very simple experiments with participants who are hard of hearing and deaf, and then analyze the results to determine which method of demonstrating accessibility is preferable for those who are hard of hearing and deaf. The researched is experiment and evaluate in Europe with German and Slovenian participants. And their language used are in Slovene language</p><p>According to the article, two different groups—one for hard of hearing participants and another for deaf participants—were separated to prevent any discrepancies data during the research methodology. Before beginning the experiment, a survey and examination need to be conducted to ensure that everyone is fully informed. There are four sections to the experiment or methods: introduction, training/practice, experiments, and evaluation:</p><p><b>Introduction:</b> Some of the participants for the tests were managed to pick and when they arrive at the research room, an introduction to the test is obligated, including a consent form and information sheet for participants. The participants must also complete their profile, including information about their age and gender (there are no restrictions for the experiment), and their proficiency with sign language and closed captions, their vision status (so they can watch videos without any issues), and their knowledge of basic Slovenian language.</p><p><b>Training/Practicing:</b> The researcher showed a short warming-up video after participants signed the consent form and indicated they were ready to continue. It serves as a learning tool for the experiments. Two videos, one in sign language with captions and the other without, will need to be watched for about ten minutes. After watching a video, participants will be asked to respond to a few questions.</p><p><b>Experiments:</b> After that, proceed to the experimental session where four videos with conversation from daily life will be shown. Each video, whether it has captions or not, includes ten follow-up questions.</p><p><b>Evaluation:</b> Participants will be asked to rate their experience watching videos with or without captions on a scale of 1 to 10, and the researcher will note their rating score and their comments for analysis purpose.</p><p>The technical equipment they used appear to have gone unmentioned in the article, but what is pretty clearly seen in figure 2 of the informational block and hiking video questionnaire, in the Background and Literature Review, is displayed by the Internet Explorer browser. A simple and direct video with open-captions: burned in captions that cannot be drag, interact or change font color and size like Youtube’s closed-captions, and a sign language interpreter is displayed on the screen. Additionally, there is an arrow button that can be likely to move the next video or questionnaire of the video.</p><p>The median number is mathematically much more accurate than the mean (average number), so in this article for the research purpose, used the median number to analyze the results while the standard deviation provides the precise interpretation of participant’s score with a distribution. The data were collected from participants 1 to 10 score survey. The data was determined using a mathematical formula in the article, and the results were shown with two findings. One finding is that the average understanding of video content by hard of hearing participants, both with open-captions and sign interpreter only, is higher than deaf participants. The second finding indicated that, among deaf and hard-of-hearing participants, the four videos with open captions collectively scored higher than the video without captions.</p><p>According to the article’s conclusion, 24% more deaf people can understand videos when there are captions and sign interpreters present. To 42% for those who have hearing impairments.</p><p>Personally, I agree with the experiment’s findings, and I recognize that other articles reporting on similar research also managed to reach the same findings. The article is concise and clear, and the majority of the text is written in plain English, which makes it easy for a non-native English speaker like me to read. The specifics of English writing, as well as the conclusion and outcomes they appear to suggest, are perfectly understandable.</p><p>After reading this article, I found it very helpful in developing the mobile app for my group project. It also answers one of my questions related to the exercise details, specifically whether the video example shown to the user is needed to provide a sign language interpreter alone or includes captions with sign interpreters. So, it’s apparent that we need both of them to ensure that our users have a higher understanding of what we’re attempting to communicate to them while also increasing accessibility for disabilities.</p>]]></content>
      
      
      <categories>
          
          <category> STM403 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Articles </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deaf Scientist</title>
      <link href="/2022/10/03/STM403/DeafScientists/"/>
      <url>/2022/10/03/STM403/DeafScientists/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/STM403/DeafScientist1.JPG"><br><img src="/images/STM403/DeafScientist2.JPG"><br><img src="/images/STM403/DeafScientist3.JPG"><br><img src="/images/STM403/DeafScientist4.JPG"><br><img src="/images/STM403/DeafScientist5.JPG"></p>]]></content>
      
      
      <categories>
          
          <category> STM403 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PPT Slides </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ethics Case Study Presentation</title>
      <link href="/2022/09/30/STM403/Scientist_Ethics/"/>
      <url>/2022/09/30/STM403/Scientist_Ethics/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/STM403/EthicalStudy,final(1).png"><br><img src="/images/STM403/EthicalStudy,final(2).png"><br><img src="/images/STM403/EthicalStudy,final(3).png"><br><img src="/images/STM403/EthicalStudy,final(4).png"><br><img src="/images/STM403/EthicalStudy,final(5).png"><br><img src="/images/STM403/EthicalStudy,final(6).png"><br><img src="/images/STM403/EthicalStudy,final(7).png"><br><img src="/images/STM403/EthicalStudy,final(8).png"><br><img src="/images/STM403/EthicalStudy,final(9).png"><br><img src="/images/STM403/EthicalStudy,final(10).png"><br><img src="/images/STM403/EthicalStudy,final(11).png"><br><img src="/images/STM403/EthicalStudy,final(12).png"><br><img src="/images/STM403/EthicalStudy,final(13).png"><br><img src="/images/STM403/EthicalStudy,final(14).png"></p>]]></content>
      
      
      <categories>
          
          <category> STM403 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PPT Slides </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ITS395 Quiz1</title>
      <link href="/2022/09/25/Accessibility/Quiz1/"/>
      <url>/2022/09/25/Accessibility/Quiz1/</url>
      
        <content type="html"><![CDATA[<ol><li><p>How can we make an image accessible to someone who can’t see it?</p><ul><li><mark class="hl-label blue">Add alternative (“alt”) text that describes the image to screen readers.</mark> </li><li>Create a separate version of the page with no images.</li><li>Pictures and photos are visual and should not be used under any circumstances.</li></ul></li><li><p>Who benefits from captions and transcripts for audio content?</p><ul><li>Deaf or hard of hearing people.</li><li>People in a noisy or quiet environment.</li><li>People with cognitive disabilities who benefit from reading text on screen while listening to it.</li><li><mark class="hl-label blue">All of the above.</mark> </li></ul></li><li><p>Which of the following is recommended for color accessibility?</p><ul><li><mark class="hl-label blue">Ensure a high level of contrast between the color of text and the background color.</mark> </li><li>Create a separate black-and-white version of the page.</li><li>Vibrant, rich color combinations should be strictly avoided, no matter what.</li><li>All of the above</li></ul></li><li><p>Certain disabilities make it difficult to understand figurative language. Providing assistance is vital for these audiences. It’s recommended with usage of an unusual or misunderstood word, to:</p><ul><li>Do nothing. The user can look up a word he/she doesn’t know</li><li><mark class="hl-label blue">Link the first instance of the word to the definition on a Web page</mark> </li><li>Define the word inline within the sentence</li><li>None of the Above</li></ul></li><li><p>Web accessibility focuses on making content accessible to</p><ul><li>People with diverse movement abilities.</li><li>People with diverse sensory abilities</li><li>People with diverse cognitive abilities</li><li><mark class="hl-label blue">All of the above</mark> </li></ul></li><li><p>How do people with different disabilities access web content?</p><ul><li><mark class="hl-label blue">With different assistive technologies (e.g., screen readers, magnifying glasses) and using different accessibility settings (e.g., contrasts, font size).</mark> </li><li>With the help of friends.</li><li>They do not access web content</li><li>With VR glasses, special computers and 3D printers</li></ul></li><li><p>Following Web Content Accessibility Guidelines (WCAG) will make:</p><ul><li>Web content boring and unattractive</li><li>Website slower</li><li><mark class="hl-label blue">Web content more usable to users in general</mark> </li><li>Website accessible in all languages</li></ul></li><li><p>Web accessibility of web content addresses:</p><ul><li>People who like to have the latest news on their electronic devices.</li><li><mark class="hl-label blue">People with various situational, temporary or permanent disabilities.</mark> </li><li>Specialists in the field of web designing.</li><li>Specialists in the field of web development.</li></ul></li><li><p>Which of the following statement is true regarding the use of color on a website?</p><ul><li><mark class="hl-label blue">Color should not be the only means of distinguishing content or conveying meaning.</mark> </li><li>Contrast is not important.</li><li>The designer can only use high contrast colors.</li><li>The designer cannot use color if the website has to be accessible.</li></ul></li><li><p>Who can provide valuable feedback in order to make the website more accessible?</p><ul><li>Elderly people.</li><li>People with disabilities</li><li>Children</li><li><mark class="hl-label blue">All of the above.</mark> </li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Accessibility Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Accessibility </tag>
            
            <tag> Quizzes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algorithms and Algorithms Analysis</title>
      <link href="/2022/08/30/Algorithm/Algorithm_Analysis/"/>
      <url>/2022/08/30/Algorithm/Algorithm_Analysis/</url>
      
        <content type="html"><![CDATA[<h3 id="What-is-an-Algorithm"><a href="#What-is-an-Algorithm" class="headerlink" title="What is an Algorithm?"></a>What is an Algorithm?</h3><p><b>Algorithm:</b> A sequence of definite instructin to perform a certian job</p><p>Example1: How to make pancakes?</p><div class="note modern"><ol><li>In a large bowl, sift together 1.5 cups of flour, 3.5 teaspoons of baking powder, 1/4 teaspoon of salt, and 1 tablespoon of sugar.</li><li>Pour in 1.25 cups of milk, 1 egg, and 3 tablespoons of melted butter.</li><li>Mix all the ingredients until smooth.</li><li>Heat a lightly oiled griddle or frying pan over medium-high heat.</li><li>Pour appromiately 1/4 cup of the batter, for each pancake, onto the griddle.</li><li>Cook until brown on both sides.</li></ol></div><p>Example2: How to withdraw money from an ATM?</p><div class="note modern"><ol><li>Place your bank card into the ATM.</li><li>Log into the ATM using your personal identification number.</li><li>Withdraw money.</li><li>Take your card back.</li><li>Take your cash.</li><li>Take your receipt.</li></ol></div><p><b>Characteristics of Algorithm:</b></p><ol><li>Definiteness of each instruction<div class="note modern"><p>We can not have an instruction that says “Add 6 or 7 to”.</p></div></li><li>Effectiveness<div class="note modern"><p>It can be done by a person using pencil and paper in a finite amount of time.</p></div></li><li>Termination<div class="note modern"><p>Terminates after a finite number of steps that can be done in a “reasonable” amount of time.</p></div></li><li>Correctness<div class="note modern"><p>The algorithm should do the job it claims to do</p></div></li><li>An algorithm should have <b>zero or more input</b> and <b>one or more outputs</b>.</li></ol><p><b>Analysis of Algorithms</b></p><p>How much time does the algorithm need: <b>Time Complexity</b><br>How much space doee the algorithm need: <b>Space Complexity</b></p><p><b>Data Structure: </b> An organization of a data set<br>Usually a data structure has several basic operations associated with it, such as search, insert, delete, etc.</p><p><b>Examples of data structures:</b></p><div class="note modern"><p>Arrays, Binary Trees, AVL Trees, Hash tables</p></div><p><b>Time Complexity Examples</b>, the following code fragment is a loop and will repeat 10 times</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>):</span><br><span class="line"><span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure><p>The following fragment is a loop will repeat <b>n</b> times</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, <span class="number">1</span>):</span><br><span class="line"><span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure><div class="note modern"><p>We say that the time complexity of this code fragment is <em>O(n)</em></p></div><h3 id="Linear-Time-Complexity"><a href="#Linear-Time-Complexity" class="headerlink" title="Linear Time Complexity"></a>Linear Time Complexity</h3><p><img src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/mypic.png" title="Where, n is the input size and c is a positive constant. "></p><table><thead><tr><th>n</th><th>Time, in second, to run the program on computer1</th><th>Time, in second, to run on computer 2</th></tr></thead><tbody><tr><td>1</td><td>10</td><td>100</td></tr><tr><td>2</td><td>20</td><td>200</td></tr><tr><td>4</td><td>40</td><td>400</td></tr><tr><td>16</td><td>160</td><td>1600</td></tr><tr><td>32</td><td>320</td><td>3200</td></tr></tbody></table><h3 id="More-Time-Complexity-Examples"><a href="#More-Time-Complexity-Examples" class="headerlink" title="More Time Complexity Examples"></a>More Time Complexity Examples</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, <span class="number">1</span>):</span><br><span class="line"><span class="built_in">print</span>(i)</span><br><span class="line"><span class="comment">#Time complexity: O(n)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, <span class="number">1</span>):</span><br><span class="line"><span class="built_in">print</span>(i)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">2</span> \*i)</span><br><span class="line"><span class="comment">#Time complexity: O(n)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>, n, <span class="number">1</span>):</span><br><span class="line"><span class="built_in">print</span>(i)</span><br><span class="line"><span class="comment">#Time complexity: O(n)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, <span class="number">2</span>):</span><br><span class="line"><span class="built_in">print</span>(i)</span><br><span class="line"><span class="comment">#Time complexity: O(n)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n\*n, <span class="number">1</span>):</span><br><span class="line"><span class="built_in">print</span>(i)</span><br><span class="line"><span class="comment">#Time complexity: O(n^2)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, <span class="number">1</span>):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, <span class="number">1</span>):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hi&quot;</span>)</span><br><span class="line"><span class="comment">#Time complexity: O(n^2)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; n:</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hi&quot;</span>)</span><br><span class="line">i = i+<span class="number">1</span></span><br><span class="line"><span class="comment">#Time complexity: O(n)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; math.sqrt(n):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hi&quot;</span>)</span><br><span class="line">i = i+<span class="number">1</span></span><br><span class="line"><span class="comment">#Time complexity: O(sqrt(n))</span></span><br></pre></td></tr></table></figure><p><a href="https://towardsdatascience.com/understanding-time-complexity-with-python-examples-2bda6e8158a7">https://towardsdatascience.com/understanding-time-complexity-with-python-examples-2bda6e8158a7</a></p>]]></content>
      
      
      <categories>
          
          <category> Algorithms </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Articles </tag>
            
            <tag> Code </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Closed Interpreting Accessibility Poster</title>
      <link href="/2022/07/21/REU/Poster/"/>
      <url>/2022/07/21/REU/Poster/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/REU_CIA/CIA_Poster.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> Accessibility Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PPT Slides </tag>
            
            <tag> REU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Closed Interpreting Accessibility</title>
      <link href="/2022/07/21/REU/FinalPaper/"/>
      <url>/2022/07/21/REU/FinalPaper/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Sign language in the media, either on-site or remotely (video interpreting) is provided to people who are Deaf and hard of hearing (DHH) to be informed about critical information (e.g. COVID-19 briefings) and other critical information in their daily lives. Access to Sign Language on TV or online media is not a requirement outlined within the American Disability Act. It is known that the DHH community relies primarily on closed captions as the most accessible option available on all media platforms. There is an increasing trend that sign language interpreting is provided exclusively in news outlets during COVID-19 lockdown (NAD vlogs, 2020) and it will continue to grow in the near future. To promote quality accessibility, we came up with a tool called AblePlayer, an accessible tool similar to closed captioning that will be displayed with an MP4 video that can be toggled on and off. The closed interpreting tool is user-adjustable. It contains settings such as: interpreter size, transparency, and location. All of which can be adjusted by the user. This study aims to evaluate and understand the current practice of media in the US and abroad. We want to eliminate all the barriers of closed captions (CC), picture in picture (PIP), and live interpreting. We hypothesize that having closed interpreting would affect the Deaf viewer’s comprehension positively. We collected feedback from 10 deaf, hearing, and hard of hearing sign language users through observational study, followed by an end-user survey. The addition of captions had a favorable effect on the participants’ comprehension rate. The most noticeable variations in understanding between viewing a sign language interpreter with no background color preference, as well as the draggable window and resize features, garnered good responses from a majority of our participants. The findings led to recommendations for the uniform usage of certified sign interpreters with no background color, draggable windows, and resize options in subtitles video across multiple media platforms.</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>The utilization of sign language interpreter videos has grown as a result of the fast development of high-speed data transmission and video compression technology. Deaf and hard of hearing people are increasingly adapting to acquire information from television shows, films, and websites. Thus, sign language interpreter films are intended for the deaf and hard of hearing who rely on sign language as a valuable tool for translating spoken and/or written words. Despite the fact that little study has been conducted on how deaf individuals perceive a recording of a sign language interpreter when watching web films, several issues are clear.</p><p>Accessibility is ‘ad-hoc’ with traditional picture in picture sign language on TV because the media tends to crop out the screen of the interpreter if it is even available to all, which prevents the sign language viewer from getting full access to information. Providing PIP on a few channels or sources achieves critical mass, albeit with limited success. If not properly set up, it might be difficult for the signing audience to see what is being spoken, resulting in incomplete communication. It only caters to the DHH community on a limited level, therefore this issue may be difficult for DHH individuals because the media is hearing-centric, and they have little experience with sign language translating on TV.<br>Captions are an alternative option to replace aural information that many deaf viewers find difficult to follow because the speed of verbatim captioning is likely to exceed their reading abilities. Even after controlling for reading level, deaf students learned less from on-screen text than hearing peers, apparently based on differences in background knowledge and information processing strategies. They prefer ASL interpreters over closed captions, partially because they find captions difficult to understand and partly because they receive less information. There are still concerns with video interpreters that must be solved.</p><p>We have outlined a few of the proposed study’s objectives. Our initial goal is to understand and learn what features help deaf, hard of hearing, and hearing sign language users. Next, we will examine how closed interpreting will assist society in including these accessibility features in mainstream accessibility standards such as closed captions. Following preliminary research, we aim to evaluate and analyze the collected data to determine how the complexity of supportive accessibility can be applied to general users, both signers and non-signers. Then we will determine which accessibility functions are the most helpful and desirable, as well as how we might improve the experience for a certain group of users.</p><h3 id="Research-Questions-Objectives"><a href="#Research-Questions-Objectives" class="headerlink" title="Research Questions/Objectives"></a>Research Questions/Objectives</h3><ul><li>In terms of design (preferred signer, signing style, background color) are these options enough for accessibility of the Deaf community?</li><li>What are the ‘new’ minimum features to be included?</li></ul><h3 id="Lit-Review-Background"><a href="#Lit-Review-Background" class="headerlink" title="Lit Review/Background"></a>Lit Review/Background</h3><p>This study aims to investigate the accessibility of media contents through sign language interpreting, which is still not widely adopted in the US. There have been several studies in the past few years (Yi, J. H. et al, 2020, Baliarda, M. B. et al. 2020, Debevc, M. et al. 2015) in Asia and Europe that have investigated the best practice of sign language interpreting in the media using PIP, and other customized features. The findings show that the PIP is essential to comprehending Sign Language users as their primary language. Additionally, those studies pursued further examination of the individual preference with the features such as sizing, placement, video backgrounds, etc). The current technology allows us to make this possible, but we do not know what the preferences are for DHH in the US. This study intends to duplicate other international studies from the DHH community to improve a better understanding of accessibility content so that DHH can conveniently access sign language in the media. The study will be designed to mainly focus on the group of participants whose primary language is American Sign Language (ASL). We call this research project ‘Closed Interpreting Accessibility’ (CIA.)</p><p>One study conducted in Korea, researched their Interpreting service among 30 Deaf and hard of hearing individuals over the age of 19. This study conducted a survey and had a selection of different types of interpreting services. Most users preferred the closed interpreting option (no background and reasonably sized). The worst screen clip out of all of the options are similar to what is currently shown on TV programs today (PIP). The experiment consisted of a total of three people: a subject, a sign language interpreter to help guide the experiment progress, and an investigator. Tobii Nano, an eye tracker, was attached to a 27-inch monitor to collect gaze information from the subject during the experiment. (Yi, J. H. et al 2020).</p><p>Another study researched how sign language users responded to a screen composition including a larger screen for the content and a smaller screen for the sign language interpreter. 32 deaf users participated in this experiment, watching four similar clips with four different screen compositions through focus group style research and surveys. The study registered the pattern of screen exploration with Eye Tracker, and assessed content recall with two questionnaires. Results show that sign language users mainly look at the sign language interpreter screen. This survey clarified the relevance of some parameters (type of on-screen insertion (picture-in-picture / half screen (split screen) / Chroma key); shot size (long shot / medium long shot / mid shot / medium close-up; interpreter’s clothing color (plain light color / plain dark color / patterned or multi- coloured); size of the interpreter’s screen (small / medium / large); on-screen positioning of the interpreter (right / left, top / center / bottom); position of the interpreter (standing / seated).) Others were considered irrelevant in terms of usability and quality of the SLI access service. For example, users considered gender, age, appearance and position of the interpreter to be of least importance. Whereas speed, size and color com- binations were the parameters that had a greater impact on screen legibility. The participants tend to look more often and for a longer time at the SLI side closer to the main screen. Results were interpreted in terms of perceptual strategies developed by Sign Language users. All participants considered that the most important on-screen parameter to grant accessibility was the size of the interpreter’s window. Most of them agreed that taking roughly a third of the split screen and using a medium shot or a medium-large shot would be ideal for news broadcasts. However, they acknowledged that it would not be appropriate for other television programs, such as interviews, films or documentaries where a larger scene screen was preferred (Baliarda, M. B. et al. 2020).</p><p>Finally, the main aim of a study conducted in Europe was to find out what the increment in comprehension of the content of sign language interpreter videos was, when captions were included. To be more specific, we compared the comprehension scores of sign language interpreter videos with or without captions. Additionally, we aimed to investigate the differences in comprehension scores between different topics (sport, hiking, shopping, and culture) presented in sign language interpreter videos with, or without, captions. Fifty-one deaf and hard of hearing sign language users alternately watched the sign language interpreter videos with, and without, captions. Afterwards, they answered ten questions. Participants were required to have the ability to read and actively understand basic written text and to have basic experience in using information and communication technology. The results showed that the presence of captions positively affected their rates of comprehension, which increased by 24% among deaf viewers and 42% among hard of hearing viewers. The results led to suggestions for the consistent use of captions in sign language interpreter videos in various media (Debevc, M. et al. 2015).</p><h3 id="Development-Setup"><a href="#Development-Setup" class="headerlink" title="Development/Setup"></a>Development/Setup</h3><p><em>Survey Setup:</em><br>We employed a screening survey at the early testing stage to qualify or exclude respondents, allowing us to ensure that respondents fit the criteria within our targeted participation group. To obtain valuable high-level responses, in-depth insights via an exit survey and interview at the end of the testing stage were conducted. This step is to obtain additional clarification information, answer participants’ confusing questions, and receive any feedback related to our research. The survey questions are based on the SUS (System Usability Scale), and participants will rank each question from 1 to 5 based on how much they agree with the statement they are reading. There are English and ASL versions available for use. Both surveys are done through Google form.</p><p><em>Able Player Setup:</em><br>We used Ableplayer as our priority supportive tool to implement the sign language interpreters and closed captions for all disability audiences. The original purpose of creating Ableplayer is to allow the front-end/full-stack developer to set up the accessible media player on the website, followed by WCAG (Web Content Accessibility Guideline) accessibility checker. The contributor AccessComputing developed this professional development of the Ableplayer project at the University of Washington with financial support from NSF. Ableplayer is an open-source tool that anyone can use, modify, and add. We removed the redundant transcripts and chapter descriptions that are the built-in features in the original code source. We are mainly focused on the sign interpreters and closed captions only. Researcher Zehui Liu developed some new features that were more accessible by using JavaScript and jQuery during the methodology development stage. The current available of the following features can be used:</p><ul><li>The media controller’s essential functions included: a timeline bar, play/pause, rewind/forward, slow/fast speed.</li><li>Built-in closed captions with user preferences provided, such as font size, text/background color, opacity, with newly added features: draggable captions in the media player.</li><li>Added features of the drop-down menu to be allowed to select multiple interpreters with various background colors. Also, an alternative option of transparency from 0% to 100% is available to interact with.</li></ul><p>The transparency option is not available all the time except the user enables interpreters features with no background. It is considered unintuitive for some users to interact with the transparency icons, but a tooltip will pop up when the user hovers that icon; it will provide a brief explanation of how to enable the transparency in some specific situation. Since this research is focused on the sign interpreters and closed captions, there is only minimum HCI of eight golden rules of the interface designed in Ableplayer.</p><p><img src="/images/REU_CIA/CIA,figure1.png" title="Figure 1: Example of Two Sign Interpreters and Closed Captions on Able Player Media"></p><p>To provide the highest quality sample videos, the two Deaf ASL interpreters pictured above, are official certified interpreters on a professional level and are actively involved in Gallaudet University’s activities for the association of sign language interpreters. The purpose for having two interpreters of different skin tones served visual contrast purposes as the DHH community heavily relies on their vision to obtain information. The interpreters were filmed in front of a chroma key background at the Kellogg Hotel Lab to illuminate the interpreters in multiple positions from the waist up and lit by using four separate lighting positions while also controlling (or eliminating entirely) the shading and shadows produced by direct lighting. The videos were captured in the high-definition resolution of 3480 x 2160 at a 29.97fps camera. Due to the lack of support for the alpha channel (the interpreting video with no background only) in MP4 format, For each interpreter’s video, Ableplayer embeds an alternate format WebM in 1280x720 resolution with no background color. Nevertheless, the original video content itself is still in MP4 with 3480 x 2160 quality for optimal message delivery.</p><p>The interpreter window is designed with size placement and other customization options on Ableplayer in mind, along with a focus on the visibility of the sign language interpreter delivering the content. We chose essential information converting economic news, interviews, and visual graphic affairs. We aimed to include other forms of entertainment to simulate our daily basis of using accessibility to see if it is possible for all forms of media to be well fit for users. The order of videos was randomized to eliminate bias that can arise out of the order in which a participant sees stimuli. The sequence of the interpreters was likewise randomized; each video topic is designated with Interpreter A or Interpreter B as a video title.</p><p><em>Testing Room Setup:</em><br>Our participants tested the tool in a research lab at Gallaudet University. A 4K monitor is connected to a laptop where all equipment is already set up for participants. A camera to film the participants and their immediate reaction, thoughts, and feedback during the usability portion of the study. This process is coined as ‘Think Aloud Protocol’ (TAP). There is also screen recording to review which functions were clicked on the most and drew the most attention to. The researchers observed the participants and provided any help and support needed with technological problems and ASL clarification. Researchers made note not to impose help in effort to not tamper with the feedback given to us.</p><h3 id="Evaluation-Study"><a href="#Evaluation-Study" class="headerlink" title="Evaluation/Study"></a>Evaluation/Study</h3><ol><li>The investigator met the participant and explained the Informed Consent and Video Release form in ASL or spoken English. A general explanation of the study purpose was to be shared as follows: “At REU AICT, we are conducting studies of different people’s user experience and feedback on closed interpreting services to promote accessibility for the Deaf and hard of hearing community.”</li><li>The participants will sit in a room with a computer provided by Gallaudet along with a camera for recording feedback.</li><li>The research assistant will explain the process on how to perform the TAP during the study by providing a video example. Additionally, they will be expected to complete a survey of Net Promoter Scale (NPS) and System Usability Scale (SUS) questions.</li><li>Before starting the study, a video camera will start recording the participant’s immediate reaction as well as a screen recording of the desktop screen that the participant will be using. The participants will be directed to the Ableplayer webpage. During the process, the participant will have access to 4 videos of different media content and 2 different signers. The different tone of the skin for vision and contrast evaluation purposes. They will spend approximately 4-5 minutes on each video. Each window will have the same accessibility functions supplied and will be permitted to access each video.</li></ol><p><img src="/images/REU_CIA/CIA,figure2.png" title="Figure 2: The  the Concept of the Participant Viewing the Able Player. "></p><p>The illustrated figure is an example of one video the participants watched and attempted to apply accessibility functions that they were most comfortable with and tested its effectiveness. There were a total of 4 videos, each video content was used from the public site platform YouTube. The footage was extracted and edited for length and uploaded in our private study webpage. Each video was played twice, once with one signer, next with the other. The participants will be asked to perform the Think Aloud Protocol (TAP) while using the platform. They are asked to describe their navigation, decision-making, and opinions during and throughout the process. The participants will fill out an end-user survey that entails a likert scale inspired by the System Usability Scale (SUS) and Net Promoter Score (NPS) followed by a short open-ended discussion.</p><p>Each participant was compensated for their time during the usability study with $25. All participants’ responses to both the usability study and survey remained confidential and was only to be shared among investigators and co-investigators of this study.</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>Overall, feedback was mostly positive. There were mixed responses about the consistency and complexity of the tool. Results from the end user survey and recordings of the participants served to examine each of their reactions to each function, such as ease of use and how quickly they can manage each function. We employ surveys and open-ended discussions for feedback on user satisfaction, comprehension of video contents, and how simple it was to see each content video and interpreter videos.<br>Participants were asked to complete the questionnaire through Google Form immediately following each usability session. With a total of ten participants in our study, we used the same survey for each round of usability testing. We used an Excel chart generation tool to visualize each participant’s responses in order to evaluate and present the data. The SUS result of each participant was processed in accordance with the SUS calculations in Figure 3.1 and the scores for individual participants and different hearing status groups of participants were examined.</p><p><img src="/images/REU_CIA/CIA,figure3.1.png" title="Figure 3.1: A Bar Chart of SUS Calculations by Score"></p><p>We gathered and documented each participant’s responses in order to create complete bar charts of SUS calculations. Combining these two dimensions produced a heat-map with positive and negative ratings, as illustrated in Figure 3.2.</p><p><img src="/images/REU_CIA/CIA,figure3.2.png" title="Figure 3.2: Detailed Diagram of SUS Calculations by a Heatmap Matrix"></p><p>The formula we used to get the SUS score (Brooke and John, 2013) per participant begins by adding the total scores for all odd-numbered survey questions (highest number, 5, to mean positive feedback). Then subtract 5 from the total to get (X). For example in Participant B:</p><table>    <tr>        <td><div align="center"> <b>X=(4+4+4+3+5)-5</b></div></td>    </tr></table><p>Then we add up the total score for all even-numbered questions (lowest number, 1, to mean positive feedback) and subtract it from 25 to get (Y).</p><table>    <tr>        <td><div align="center"> <b>Y=25–(1+1+2+1+1)</b></div></td>    </tr></table><p>Finally, we multiply the total score of the new values (X+Y) by 2.5.</p><table>    <tr>        <td><div align="center"> <b><i>Participant B SUS score</i> = X+Y*2.5</b></div></td>    </tr></table><p>There is only one participant score computed above; When all of our (10) participants’ scores are aggregated, the average SUS score is 82.2, followed by the System Usability Acceptability Score, which is in the acceptable range but close to the excellent rating for good design. It is worth noting that all of the participants fully agree with the system and are quite confident in using it frequently. Both the SUS and System Usability Acceptability, received 49 points out of a possible 50 on survey questions 1 and 9, accounting for 98 percent of the highest rate in the study.</p><p>In the open-ended discussion portion of the study, the majority of the participants agreed with the statement that Ableplayer was a useful tool and can be applied to our real life on a daily basis on each media player. Only a small number of respondents indicated that they would still prefer to use traditional PIP (Picture in Picture) or live captions similar to Youtube auto captions for their satisfaction of accessibility. There are some negative feedbacks about the buggy, unreachable functions are not consistent to interact with is their major concern and providing several suggestions such as providing a professional team to develop with more UI and UX testing.<br>When asked about the customization of captions preference, half of our participants unanimously agreed that it is necessary to exist for more flexibility. Comments entailed:</p><table>    <tr>        <td><i>“I see why your tool needs to offer captions preferences, and it is sometimes necessary for … colorblind viewers, who have difficulty seeing a specific color and must alter their best color for watching experience.”</i></td>    </tr></table><table>    <tr>        <td><i>“I like the captions preference options… (but I) don't like the white font with black background, so I simply change the background to green, now it works much better for me.”</i></td>    </tr></table><p>The customizable implementation of sign interpreter window had more positive response than the closed captions preference. One participant commented:</p><table>    <tr>        <td><i>“The draggable signer window is fantastic…However, I believe the drop-down choice is worded incorrectly…(and) you mixed in the signer enable options and background color…(So) consider separating them and giving them their own icons and functions on the media controller.”</i></td>    </tr></table><p>Taken together, these findings suggest that there is an association between the complexity of the user interface as well as its unfriendliness to inexperienced computer users. Some participants took longer than others to figure out how the tool worked and needed further guidance and explanation for the research itself. Eventually they managed to figure it out on their own. The sign interpreter window received far more positive feedback, but it required further testing and tweaking on the drop-down menu to provide the optimal message to the user.</p><p>The next section of the open-ended discussion is the transparency function. We noticed that it might not have been the most functional part of the software on specific areas of certain videos. When the transparency of sign interpreter windows with no background color were mentioned, two third of our participants gave it a thumbs down to the transparency because of the busy background that made it hard for the user to see. Furthermore, when clicking on a background screen color for the interpreter, transparency was not available to serve that function.</p><p><img src="/images/REU_CIA/CIA,figure3.3.png" title="Figure 3.3: 50% of Transparency of Each Sign Interpreter "></p><p>In the figure 3.3 above where the interpreter on the left is mixed with white font, it is difficult to see what the sign interpreter is signing. Additionally, the interpreter on the right with a dark busy background, is just as very difficult to decipher. Another participant, when asked for the transparency function, said:</p><table>    <tr>        <td><i>“It (transparency) looks cool at first, but when I enter Fullscreen mode and activate it with 50% opacity to apply the Oscar video and another one with gas prices in Los Angeles, it is difficult to view. So, in my perspective, I do not recommend providing transparency.”</i></td>    </tr></table><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>A new form of media accessibility for the Deaf and hard of hearing community has been necessary and crucial. We hoped to have not only reached that new form, but created a new standard of the needs of the community. For a long time, the DHH community has been left out of information and communication in the media. Ableplayer aims to give the community better access to information and communication from a variety of media beyond news programs and give the user control and options to accommodate best to their language needs.</p><p>We found that many people did indeed like the customizable closed interpreting implementation over the traditional interpreting service. However, there were several survey results and feedback that contradicted each other. In general, there was a noticeable and significant increase in satisfaction, understanding and ease of viewing from participant’s feedback. Participants also preferred the interpreting features that allow for resizing and relocating the interpreter video in the customizable implementation, but the transparency feature was not well-received.</p><p>There will always be more that can be achieved when it comes to accessibility for the Deaf and hard of hearing community, the solution is not one-size fits all. Our tool brought something new to the table and was able to create discourse on how it can improve to achieve greater quality accessibility.</p><h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><p>Some limitations to consider for future work, the data we obtained contains only 10 participants for the time being, all of who were Deaf, hard of hearing or hearing and ages 18-34 years. There is a need to obtain a larger sample size and demographic variability so that the data can be generalizable. Some survey questions were difficult for participants to comprehend, and only a few Deaf people responded without asking for explanation in ASL. Led to nonsensical answers such as repeatedly choosing the answer ‘5’ to mean, strongly agree for all questions. We had originally planned to provide an ASL version on each survey question, however due to the time constraint, we were only able to provide in-person clarification given that the participant asked for it. There was potential researcher bias in that explanation of the testing process to each participant slightly differed as some participants asked for more clarification even though the research was aimed to study effectiveness and understanding of the tool independent from guidance.</p><p>Some participants were curious as to why there was a need for two interpreters of different races. The intention mainly served visual contrast purposes particularly for Deaf-blind users as well as the DHH community as a whole, due to the fact that the Deaf community in general is largely visually reliant in receiving information. It was taken into consideration that all of the participants more than likely understood the function to serve as a cultural representation purpose as both signers had different signing styles and dialects. For this reason, the demand for more diversity in this function was mentioned frequently and brought more questions. It would be necessary to have a variety of certified trained sign interpreters to record in order to add diverse signers of different nationalities, colors, and genders solely for representation purposes.</p><p>It is important to note some people found it hard to catch all the information from videos that had two people speaking. There was only one interpreter provided, switching back and forth between speakers and interpreting for two people. In the future we should work towards having more than one interpreter ready to ease communication and information delivery.</p><p>In the tool, the sign interpreter with no background color is achieved by exporting an alpha channel with WebM video format using the Premiere Pro video editor. It is possible to automate the conversion of video to no background by uploading the video clip to the ableplayer website. This is the most ideal solution in order to save more time and less workload. There were technological concerns that necessitated the assistance of additional UI and UX professional consultants and specialists, such as incorporating a new design style on a dropdown menu. A professional can provide far more beneficial recommendations. While working on this enormous project since 2014, more than three or four individuals are also encouraged to contribute to the future development of Ableplayer.</p><h3 id="Resources-Availability"><a href="#Resources-Availability" class="headerlink" title="Resources Availability"></a>Resources Availability</h3><p>All research data (including paper and electronic) will be treated with the utmost respect and confidentiality. All data collected and downloaded will be securely stored in the cloud drive with password protection and will not be shared with others who aren’t listed in this IRB study approved, the principal co-investigators are (Bridget Lam, Zehui Liu) and program mentors are (Dr. Boudreault, Dr. Vogler and Dr. Kushalngaer) All of the sensitive data will be appropriately disposed of after the content is no longer needed or up to 7 years. There will be no identifying data that will be discussed about the participants in reporting the results.</p><p>The Ableplayer tool is available on <a href="https://github.com/ahui9605/ableplayer-example">GitHub</a>, but only the source code for it is provided; the video footages, sign interpreters, and captions that we use are not available at this time<br>Please see the manul on how to set up the Able Player tool for educational purposes: <a href="https://github.com/ahui9605/ableplayer-example">Click here</a></p><h3 id="Acknowledgements"><a href="#Acknowledgements" class="headerlink" title="Acknowledgements"></a>Acknowledgements</h3><p>This work has been generously supported by NSF REU Site Grants (#1757836 and #2150429) awarded to Dr. Raja Kushalnagar, PI and Dr. Christian Vogler, Co-PI.<br>Special thanks to Lizzie from RIT with many supports and ideas on Ableplayer<br>Special thanks to Afzal with technical assistance on jQuery and JavaScript during the development stage of Ableplayer</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><table>    <tr>        <td>Bosch-Baliarda, M., Soler-Vilageliu, O., & Orero, P. (2020). Sign language interpreting on TV: A reception study of visual screen exploration in deaf signing users. MonTI. Monografías De Traducción e Interpretación, (12), 108–143. https://doi.org/10.6035/monti.2020.12.04        </td>    </tr>        <tr>        <td>Debevc, M., Milošević, D., & Kožuh, I. (2015). A comparison of comprehension processes in sign language interpreter videos with or without captions. PLOS ONE, 10(5). https://doi.org/10.1371/journal.pone.0127577        </td>    </tr>        <tr>        <td>Cronin, B. J. (2013, April 22). Chapter 14: Closed-caption television: Today and Tomorrow. American Annals of the Deaf. Retrieved June 7, 2022, from https://muse.jhu.edu/article/386799/pdf        </td>    </tr>        <tr>        <td>Huang, C.-wei. (2003). Automatic Closed Caption Alignment Based on Speech Recognition Transcript. CiteSeerX. Retrieved June 7, 2022, from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.233.419        </td>    </tr>            <tr>        <td>Yi, J. H., et al. (2021). Design Proposal for Sign Language Services in TV Broadcasting from the Perspective of People Who Are Deaf or Hard of Hearing. Applied Sciences, 11(23), 11211. https://doi.org/10.3390/app112311211        </td>    </tr>            <tr>        <td>Brooke, John. (2013). SUS: a retrospective. Journal of Usability Studies. 8. 29-40.        </td>    </tr>            <tr>        <td>Brooke, J. (1986). “SUS: a “quick and dirty” usability scale”. In P. W. Jordan, B. Thomas, B. A. Weerdmeester, & A. L. McClelland (eds.). Usability Evaluation in Industry. London: Taylor and Francis.        </td>    </tr>            <tr>        <td>https://www.youtube.com/watch?v=Z2O3mYNxjHM. (2022). [Youtube Video].        </td>    </tr></table>]]></content>
      
      
      <categories>
          
          <category> Accessibility Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Articles </tag>
            
            <tag> REU </tag>
            
            <tag> Web Development </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MS Server Test1</title>
      <link href="/2022/03/05/MS%20Server/test1/"/>
      <url>/2022/03/05/MS%20Server/test1/</url>
      
        <content type="html"><![CDATA[<h3 id="MAIN"><a href="#MAIN" class="headerlink" title="MAIN"></a>MAIN</h3><p><strong>1. A windows server uses an IP address 36.128.22.234. What is the socket address for the machine’s web server when it allows Secure HTTP connections?</strong><br>The socket is 36.128.22.234:443 where 36.128.22.234 is the IP address and the 443 is the HTTPS port” blue</p><p><strong>2. Which of the following is a private IP address?</strong></p><ul><li>197.28.112.256</li><li>172.22.265.12</li><li>147.123.18.28</li><li><mark class="hl-label blue">10.1.12.213</mark> </li></ul><p><strong>3. What is the main purpose of Dynamic Host Control Protocol (DHCP) on a server? (Explain in your own words giving an example you see when starting and stopping your Google VM. If you copy-paste answers from generic google search no credit is given)</strong></p><p>These features can be added to Microsoft Windows Server through the server roles. On the server roles window, an option shows DHCP Server; according to the description, the aim of using the DHCP is to allow the user to configure, maintain, and distribute temporary IP numbers and other client-side information.”</p><p><strong>4. What is the output of following Power Shell script? Run line by line in Power Shell command line window (NOT Power Shell ISE) and attach a screenshot.</strong></p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> c:\users\&lt;yourname&gt;\desktop <span class="comment"># should show like C:\Users\danturthi\desktop&gt;</span></span><br><span class="line"><span class="built_in">new-Item</span> abcd.txt</span><br><span class="line"><span class="built_in">Add-Content</span> abcd.txt “This is the first line”</span><br><span class="line"><span class="built_in">Add-Content</span> abcd.txt <span class="literal">-value</span>(<span class="built_in">get-content</span> abcd.txt)</span><br><span class="line"><span class="built_in">write-output</span> “The file contents are:”</span><br></pre></td></tr></table></figure><p><strong>Answer this question - Which of the above commands is unnecessary/Superfluous (can be completely removed) in this particular example to achieve the same output?</strong></p><p><img src="/images/MS/MS-4.png" title="I remove “Add-Content abcd.txt -value(get-content abcd.txt)” because it shows duplicated content in the powershell which is redundant and unnecessary."></p><p><strong>5. In the windows server manager, you want to see the hard disk allocation and the available disk partitions or space. What options do you pick from Windows server manager window? Produce a screenshot on your VM that shows the disks and paste it here.</strong></p><p><img src="/images/MS/MS-5.png" title="Open the Server Manager application, move the pointer to the left sidebar, and look for the File and Storage Services option. Then click the Volumes, available disk partitions and spaces will be display at the center of the screen."></p><p><strong>6. When we do a scan for Best Practices Analyzer (BPA) on Windows Server 2019, what does it indicate for each role configuration?</strong></p><ul><li>A. If the role configuration falls through Microsoft’s control lists</li><li>B. Whether the role configuration violates any existing practices</li><li><mark class="hl-label blue">C. If the role configuration meets Microsoft’s minimum guidelines</mark> </li><li>D. Whether firewall is actually working on the role configuration</li></ul><p><strong>7. A programmer writes code - that is supposed to use a system account, but uses his own account (username and password) to test an application on a development machine. When all development and testing goes well, he transfers the code without any changes to you, the System Administrator (SA). When you deploy the code on production server, the code fails to run and says “incorrect username and password.” As the SA what do you think is the problem and how/what would you advise the programmer to fix the application?</strong></p><p>Before running a test on a production server, applications must modify their username and settings. Even the developer was able to successfully execute the applications on his development PC.</p><p><strong>8. Copy the following code to Power Shell ISE into file okay.ps1, save it and run it. Put your name below for correct path for your VM and Take a screen shot and enclose results of the script here and answer the following questions</strong></p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> c:\users\&lt;yourname&gt;\desktop <span class="comment"># should be like C:\Users\danturthi\desktop&gt;</span></span><br><span class="line"><span class="built_in">Add-Content</span> myFile.txt <span class="string">&quot;`nName `tQ#1 `tT#1 `tGrade&quot;</span></span><br><span class="line"><span class="built_in">Add-Content</span> myFile.txt <span class="string">&quot;`nDavid `t45 `t110 `t155&quot;</span></span><br><span class="line"><span class="built_in">Add-Content</span> myFile.txt <span class="string">&quot;`nMelissa `t40 `t120 `t160&quot;</span></span><br><span class="line"><span class="built_in">Add-Content</span> myFile.txt <span class="string">&quot;`nRick `t50 `t90 `t140&quot;</span></span><br><span class="line"><span class="built_in">Get-Content</span> myFile.txt | <span class="built_in">select-string</span> <span class="string">&#x27;a&#x27;</span>| <span class="built_in">write-output</span></span><br></pre></td></tr></table></figure><p><img src="/images/MS/MS-8.png"></p><ul><li><p><strong>(i) Explain what the “ select-string ‘a’ “ is doing</strong><br>select-string ‘a’ (Select all the string values that included the letter ”a” from the file named “myFile.txt”) |write-output (output the selected string values and display on the screen from the powershell)</p></li><li><p><strong>(ii) What are the characters “ <code>t “ and “ </code>n “ doing in the power shell script?</strong><br>“<code>t“ means horizontal tab and “</code>n” means make a new line.</p></li><li><p><strong>(iii) What happens if you remove all the “ `n “ characters from power shell script?</strong><br>Powershell does not add any new lines to the output.</p></li></ul><p><img src="/images/MS/MS-8_2.png"></p><div class="note modern warning simple"><p>NOTE: The character before “n” and “t” is the backtick (next to numeric 1)</p></div><p><strong>9. On VM you open the command prompt in Power Shell window and execute the following command (If IIS is not installed, go to control panel -&gt; programs and features and -&gt; install Internet Information Server)</strong></p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-nettcpconnection</span> <span class="literal">-Localport</span> <span class="number">56</span></span><br><span class="line">and you notice you are getting an error. Replace the port <span class="number">56</span> with <span class="number">80</span> and run again.</span><br><span class="line">Now run this command: netstat <span class="literal">-an</span> | <span class="built_in">select-string</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p><strong>What do you conclude from the results of these three commands about ports 56 and 80? (Attach a screen shot)</strong></p><p>After installed the IIS in the VM, I am unable to get the TCP connection and view its property when I attempt the connection of localport 56 because this port could not be found. However, I am able to see the tcp connection of port 80, and its status is listening, also some established tcp connections.</p><p><img src="/images/MS/MS-9.png"></p><p><strong>10. Stateful Packet Inspection (SPI) of a firewall is used for examining packets as they move in and out of a network.</strong><br><strong>What is the main focus of SPI?</strong><br>SPI mainly focused on connections</p><p><strong>What are rules SPI implements for internal and external connections?</strong><br>Internally connection allow by default and externally connection prohibit by default.</p><ol start="11"><li>From the VM, choose google SDK command line interpreter and execute the following commands:</li></ol><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gsutil du c:</span><br><span class="line">gsutil du gs://&lt;your bucketname&gt;</span><br><span class="line">gsutil du</span><br></pre></td></tr></table></figure><p><strong>Take a screen shot of the results (individually or together)and explain what it is displaying. NOTE: These commands may not work on Power Shell or DOS window</strong></p><p><img src="/images/MS/MS-11.png"></p><p><strong>12. What three kinds of rules can you set up with Windows Defender Firewall on the Windows Server 2019 (Google Virtual Machine)?</strong></p><ul><li>(1) Inbound Rules</li><li>(2) Outbound Rules</li><li>(3) Connection Security Rules</li></ul><p><strong>13. Open Google VM server in remote desktop, choose run and enter the following – Sigverif in the run window. When it shows the actual application, run it and click “Start” on the application. When it is finished, attach the resulting log file contents to this test (add another page, if required)</strong></p><p><img src="/images/MS/MS-13.png" title="Here is the screenshot of the log file after finishing the scan."></p><p><strong>14. When a firewall examines packets as a part of stateful packet inspection logic, what are the two main categories a packet state can fall into?</strong></p><ul><li>A. Packets opening connections and packets closing connections for data move</li><li>B. Packets trying to send data and packets trying to receive data from connections</li><li><mark class="hl-label blue">C. Packets trying to open connections and packets not trying to open connections</mark> </li><li><mark class="hl-label blue">D. Packets that are static with same IP and packets that are dynamic with variable IP</mark> </li></ul><p><strong>15. Your Network/Server has a border firewall to block/deny any suspicious attack packets. Besides blocking the packets, what must the firewall do for the system administrator?</strong></p><p>Firewall not just only process the regular check to block packages, but also can drop and update logs, caution the SA if there is any suspicious action appear</p><h3 id="BONUS"><a href="#BONUS" class="headerlink" title="BONUS"></a>BONUS</h3><p><strong>16. Your firewall has a 276 rules you created in addition to the default rule to examine and allow or deny packets. As a System Administrator you are given the job of arranging these access control rules in a numerical list known as Access Control List (ACL).</strong></p><p><strong>A. Which rule will be your first rule and which rule will be your last rule in the ACL? (Just mention how you pick the first rule and last rule and on what criteria).</strong><br>The first rule will be creating a set of rules that the firewall may use to monitor various packages as the priority. The default rule usually will be the last execution in the rules list.</p><p><strong>B. What happens if you swap (exchange) the first rule to the last?</strong><br>Swapping the rule to the default rule first may not be a smart idea since if the default rule is successful, the rest of the rules will not be executed.</p>]]></content>
      
      
      <categories>
          
          <category> MS Server </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Exams </tag>
            
            <tag> Code </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
